{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_full_text(tweet):\n",
    "    if 'extended_tweet' in tweet:\n",
    "        return tweet['extended_tweet']['full_text']\n",
    "    else:\n",
    "        return tweet['text']\n",
    "    \n",
    "def clean_up_text(text):\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',' ')\n",
    "    return text.strip()\n",
    "        \n",
    "def get_orig_text(tweet):\n",
    "    if 'retweeted_status' in tweet:\n",
    "        return get_orig_text(tweet['retweeted_status'])\n",
    "    else:\n",
    "        return clean_up_text(get_full_text(tweet))\n",
    "    \n",
    "directory = [f for f in glob.iglob('Downloads/BackLAOut/*')]\n",
    "i = 0\n",
    "raw_tweets = []\n",
    "for filepath in directory:\n",
    "    if len(raw_tweets) > 2e5: break # comment out when running full dataset\n",
    "\n",
    "    if (i > 0) and (i % (len(directory)//10) == 0):\n",
    "        print(str(i)+' of '+str(len(directory))+' files read, '+\n",
    "              str(len(raw_tweets))+' total tweets')\n",
    "    file = open(filepath, 'r')\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        text = get_orig_text(tweet)\n",
    "        if ('http://' not in text) and ('https://' not in text):\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            raw_tweets.append(text)\n",
    "    file.close()\n",
    "    i += 1\n",
    "raw_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = list(set(raw_tweets))\n",
    "joined_tweets = ' '.join(raw_tweets)\n",
    "\n",
    "len(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "raw_word_counts = Counter()\n",
    "raw_word_list = joined_tweets.split(' ')\n",
    "for word in raw_word_list:\n",
    "    raw_word_counts[word] += 1\n",
    "    \n",
    "def display_top_10_counts(d):\n",
    "    odct = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    for i, k in enumerate(list(odct)):\n",
    "        if i > 9:\n",
    "            del odct[k]\n",
    "    return odct\n",
    "\n",
    "display_top_10_counts(raw_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_char_counts = Counter()\n",
    "raw_char_list = [char for char in joined_tweets]\n",
    "for char in raw_char_list:\n",
    "    raw_char_counts[char] += 1\n",
    "\n",
    "display_top_10_counts(raw_char_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "def split_train_valid_test(X,y=[]):\n",
    "    percent_train = 0.9\n",
    "    test_size = 50\n",
    "\n",
    "    train_size = batch_size*int(percent_train*len(X)/batch_size)\n",
    "    valid_size = len(X) - test_size - train_size\n",
    "    assert(valid_size > 0)\n",
    "\n",
    "    X_valid, y_valid = X[:valid_size], y[:valid_size]\n",
    "    X_train, y_train = X[valid_size:-test_size], y[valid_size:-test_size]\n",
    "    X_test, y_test = X[-test_size:], y[-test_size:]\n",
    "#     assert((len(X_train)==len(y_train)) and\n",
    "#            (len(X_valid)==len(y_valid)) and\n",
    "#            (len(X_test)==len(y_test)))\n",
    "    print(len(X_train)/len(X),len(X_valid)/len(X),len(X_test)/len(X),len(X_train)/batch_size)\n",
    "    if y==[]:\n",
    "        return X_train,X_valid,X_test\n",
    "    else:\n",
    "        return X_train,X_valid,X_test,y_train,y_valid,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_train,raw_tweets_valid,raw_tweets_test = split_train_valid_test(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sentiment = {'â˜¹': -1,\n",
    "                   'â˜º':  1,\n",
    "                   'â™¥':  1,\n",
    "                   'â£':  1,\n",
    "                   'â¤':  1,\n",
    "                   'ðŸ±':  1,\n",
    "                   'ðŸµ':  0,\n",
    "                   'ðŸ‘…':  1,\n",
    "                   'ðŸ‘º': -1,\n",
    "                   'ðŸ‘¿': -1,\n",
    "                   'ðŸ’‘':  1,\n",
    "                   'ðŸ’“':  1,\n",
    "                   'ðŸ’”': -1,\n",
    "                   'ðŸ’•':  1,\n",
    "                   'ðŸ’–':  1,\n",
    "                   'ðŸ’—':  1,\n",
    "                   'ðŸ’˜':  1,\n",
    "                   'ðŸ’™': -1,\n",
    "                   'ðŸ’š':  1,\n",
    "                   'ðŸ’›':  1,\n",
    "                   'ðŸ’œ':  1,\n",
    "                   'ðŸ’':  1,\n",
    "                   'ðŸ’ž':  1,\n",
    "                   'ðŸ’Ÿ':  1,\n",
    "                   'ðŸ–¤': -1,\n",
    "                   'ðŸ˜€':  1,\n",
    "                   'ðŸ˜':  0,\n",
    "                   'ðŸ˜‚':  0,\n",
    "                   'ðŸ˜ƒ':  1,\n",
    "                   'ðŸ˜„':  1,\n",
    "                   'ðŸ˜…':  0,\n",
    "                   'ðŸ˜†':  0,\n",
    "                   'ðŸ˜‡':  1,\n",
    "                   'ðŸ˜ˆ':  1,\n",
    "                   'ðŸ˜‰':  1,\n",
    "                   'ðŸ˜Š':  1,\n",
    "                   'ðŸ˜‹':  1,\n",
    "                   'ðŸ˜Œ':  1,\n",
    "                   'ðŸ˜':  1,\n",
    "                   'ðŸ˜Ž':  1,\n",
    "                   'ðŸ˜':  1,\n",
    "                   'ðŸ˜':  0,\n",
    "                   'ðŸ˜‘': -1,\n",
    "                   'ðŸ˜’':  0,\n",
    "                   'ðŸ˜“': -1,\n",
    "                   'ðŸ˜”': -1,\n",
    "                   'ðŸ˜•': -1,\n",
    "                   'ðŸ˜–': -1,\n",
    "                   'ðŸ˜—':  1,\n",
    "                   'ðŸ˜˜':  1,\n",
    "                   'ðŸ˜™':  1,\n",
    "                   'ðŸ˜š':  1,\n",
    "                   'ðŸ˜›':  1,\n",
    "                   'ðŸ˜œ':  1,\n",
    "                   'ðŸ˜':  1,\n",
    "                   'ðŸ˜ž': -1,\n",
    "                   'ðŸ˜Ÿ': -1,\n",
    "                   'ðŸ˜ ': -1,\n",
    "                   'ðŸ˜¡': -1,\n",
    "                   'ðŸ˜¢': -1,\n",
    "                   'ðŸ˜£': -1,\n",
    "                   'ðŸ˜¤': -1,\n",
    "                   'ðŸ˜¥': -1,\n",
    "                   'ðŸ˜¦': -1,\n",
    "                   'ðŸ˜§': -1,\n",
    "                   'ðŸ˜¨': -1,\n",
    "                   'ðŸ˜©': -1,\n",
    "                   'ðŸ˜ª':  0,\n",
    "                   'ðŸ˜«': -1,\n",
    "                   'ðŸ˜¬':  0,\n",
    "                   'ðŸ˜­': -1,\n",
    "                   'ðŸ˜®':  0,\n",
    "                   'ðŸ˜¯':  0,\n",
    "                   'ðŸ˜°': -1,\n",
    "                   'ðŸ˜±':  0,\n",
    "                   'ðŸ˜²': -1,\n",
    "                   'ðŸ˜³': -1,\n",
    "                   'ðŸ˜´':  0,\n",
    "                   'ðŸ˜µ': -1,\n",
    "                   'ðŸ˜¶':  0,\n",
    "                   'ðŸ˜·':  0,\n",
    "                   'ðŸ˜¸':  1,\n",
    "                   'ðŸ˜¹':  0,\n",
    "                   'ðŸ˜º':  1,\n",
    "                   'ðŸ˜»':  1,\n",
    "                   'ðŸ˜¼':  1,\n",
    "                   'ðŸ˜½':  1,\n",
    "                   'ðŸ˜¾': -1,\n",
    "                   'ðŸ˜¿': -1,\n",
    "                   'ðŸ™€':  0,\n",
    "                   'ðŸ™': -1,\n",
    "                   'ðŸ™‚':  1,\n",
    "                   'ðŸ™ƒ':  1,\n",
    "                   'ðŸ™„':  0,\n",
    "                   'ðŸ™ˆ':  0,\n",
    "                   'ðŸ™‰':  0,\n",
    "                   'ðŸ™Š':  0,\n",
    "                   'ðŸ¤':  1,\n",
    "                   'ðŸ¤Ž':  1,\n",
    "                   'ðŸ¤':  0,\n",
    "                   'ðŸ¤‘':  1,\n",
    "                   'ðŸ¤’': -1,\n",
    "                   'ðŸ¤“':  0,\n",
    "                   'ðŸ¤”':  0,\n",
    "                   'ðŸ¤•': -1,\n",
    "                   'ðŸ¤–':  0,\n",
    "                   'ðŸ¤—':  1,\n",
    "                   'ðŸ¤›':  1,\n",
    "                   'ðŸ¤›ðŸ»':  1,\n",
    "                   'ðŸ¤›ðŸ¼':  1,\n",
    "                   'ðŸ¤›ðŸ½':  1,\n",
    "                   'ðŸ¤›ðŸ¾':  1,\n",
    "                   'ðŸ¤›ðŸ¿':  1,\n",
    "                   'ðŸ¤œ':  1,\n",
    "                   'ðŸ¤œðŸ»':  1,\n",
    "                   'ðŸ¤œðŸ¼':  1,\n",
    "                   'ðŸ¤œðŸ½':  1,\n",
    "                   'ðŸ¤œðŸ¾':  1,\n",
    "                   'ðŸ¤œðŸ¿':  1,\n",
    "                   'ðŸ¤ ':  1,\n",
    "                   'ðŸ¤¡':  1,\n",
    "                   'ðŸ¤¢': -1,\n",
    "                   'ðŸ¤£':  0,\n",
    "                   'ðŸ¤¤':  1,\n",
    "                   'ðŸ¤¥': -1,\n",
    "                   'ðŸ¤¦': -1,\n",
    "                   'ðŸ¤¦ðŸ»': -1,\n",
    "                   'ðŸ¤¦ðŸ¼': -1,\n",
    "                   'ðŸ¤¦ðŸ½': -1,\n",
    "                   'ðŸ¤¦ðŸ¾': -1,\n",
    "                   'ðŸ¤¦ðŸ¿': -1,\n",
    "                   'ðŸ¤§':  0,\n",
    "                   'ðŸ¤¨':  0,\n",
    "                   'ðŸ¤ª':  0,\n",
    "                   'ðŸ¤«':  0,\n",
    "                   'ðŸ¤¬': -1,\n",
    "                   'ðŸ¤­':  1,\n",
    "                   'ðŸ¤®': -1,\n",
    "                   'ðŸ¤¯':  0,\n",
    "                   'ðŸ¥°':  1,\n",
    "                   'ðŸ¥±':  0,\n",
    "                   'ðŸ¥³':  1,\n",
    "                   'ðŸ¥´':  1,\n",
    "                   'ðŸ¥µ': -1,\n",
    "                   'ðŸ¥¶': -1,\n",
    "                   'ðŸ¥º': -1,\n",
    "                   'ðŸ§':  0,\n",
    "                   'ðŸ§¡':  1}\n",
    "def clean_tweets(X):\n",
    "    cleaned_tweets = []\n",
    "    i = 0\n",
    "    for i, origtweet in enumerate(X):\n",
    "        i += 1\n",
    "        if i % (len(X)//10) == 0:\n",
    "            print(str(10*i/(len(X)//10))+'% read')\n",
    "\n",
    "        emojis = []\n",
    "        tweet = ''\n",
    "        no_emojis = True\n",
    "        for e in emoji_sentiment:\n",
    "            if e in origtweet:\n",
    "                no_emojis = False\n",
    "                for char in origtweet:\n",
    "                    if char == e:\n",
    "                        tweet += ' '+char+' ' # this is so we can parse out the emoji\n",
    "\n",
    "                        emojis.append(e)\n",
    "                    else:\n",
    "                        tweet += char\n",
    "        if no_emojis:\n",
    "            continue\n",
    "\n",
    "        tweet_cleaned = []\n",
    "        for word in clean_up_text(tweet).split(' '):\n",
    "            if word[0] != '@':\n",
    "                tweet_cleaned.append(word)\n",
    "        tweet_cleaned = ' '.join(tweet_cleaned)\n",
    "\n",
    "        cleaned_tweets.append(tweet_cleaned)\n",
    "        \n",
    "    return list(set(cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets_train = clean_tweets(raw_tweets_train)\n",
    "cleaned_tweets_valid = clean_tweets(raw_tweets_valid)\n",
    "cleaned_tweets_test = clean_tweets(raw_tweets_test)\n",
    "\n",
    "len(cleaned_tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get__X_and__y(cleaned_tweets):\n",
    "    _X = []\n",
    "    _y = []\n",
    "    not_emojis = 0\n",
    "    for i, tweet in enumerate(cleaned_tweets):\n",
    "        sentiments = []\n",
    "        not_emoji = False\n",
    "        for e in emoji_sentiment:\n",
    "            if 'not '+e in tweet:\n",
    "                not_emoji = True\n",
    "        if not_emoji:\n",
    "            not_emojis += 1\n",
    "            continue\n",
    "        emojis = []\n",
    "        for word in tweet.split(' '):\n",
    "            for e in emoji_sentiment:\n",
    "                if word == e:\n",
    "                    sentiments.append(emoji_sentiment[e])\n",
    "                    emojis.append(e)\n",
    "        # only include tweets that have exactly one of the selected emojis\n",
    "        if len(set(sentiments)) == 1:\n",
    "            for e in emojis:\n",
    "                tweet = tweet.replace(e,'')\n",
    "            _X.append(clean_up_text(tweet))\n",
    "            _y.append(sentiments[0])\n",
    "    return _X, _y, not_emojis\n",
    "_X_train, _y_train, not_emojis_train = get__X_and__y(cleaned_tweets_train)\n",
    "_X_valid, _y_valid, not_emojis_valid = get__X_and__y(cleaned_tweets_valid)\n",
    "_X_test, _y_test, not_emojis_test = get__X_and__y(cleaned_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def get_most_frequent_keys(d,n):\n",
    "#     a = np.array(list(d.values()))\n",
    "#     thresh = min(a[np.argpartition(a,-n)][-n:])\n",
    "#     inds = a > 0.9 * thresh\n",
    "#     d = dict(np.array(list(d.items()))[inds])\n",
    "#     for k in d:\n",
    "#         d[k] = int(d[k])\n",
    "#     odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "#     return odct\n",
    "\n",
    "# n_classes = 10\n",
    "# most_frequent_emojis_train = get_most_frequent_keys(emoji_counts_train, n_classes)\n",
    "# most_frequent_emojis_valid = get_most_frequent_keys(emoji_counts_valid, n_classes)\n",
    "# most_frequent_emojis_test = get_most_frequent_keys(emoji_counts_test, n_classes)\n",
    "# n_classes = len(most_frequent_emojis_train)\n",
    "# most_frequent_emojis_train,len(most_frequent_emojis_train)\n",
    "\n",
    "emoji_counts = {-1:0,0:0,1:0}\n",
    "\n",
    "for i in _y_train:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "words_list = ' '.join(_X_train+_X_valid+_X_test).split(' ')\n",
    "word_counts = Counter()\n",
    "for word in words_list:\n",
    "    word_counts[word] += 1\n",
    "    \n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    return to_categorical(_y, num_classes=3)\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "X_train = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_train]\n",
    "X_valid = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_valid]\n",
    "X_test = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_test]\n",
    "\n",
    "max_words = 140\n",
    "X_train= sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "y_train = convert_to_one_hot(_y_train)\n",
    "y_valid = convert_to_one_hot(_y_valid)\n",
    "y_test = convert_to_one_hot(_y_test)\n",
    "\n",
    "X_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.arange(3)-1, y=_y_train)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "def calc_n_neurons():\n",
    "    alpha = np.log(len(X_train))\n",
    "    Ni = model.layers[-1].input.shape[-1]\n",
    "    No = model.layers[-1].output.shape[-1]\n",
    "    n_neurons = max(int(2 * len(X_train) / (alpha * (Ni + No))), 1)\n",
    "    print(alpha,Ni,No,n_neurons)\n",
    "    return n_neurons\n",
    "\n",
    "model.add(LSTM(calc_n_neurons(), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(calc_n_neurons()))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 25\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for word,i in word2id.items()}\n",
    "\n",
    "model.load_weights(filepath)\n",
    "for i,x in enumerate(X_test):\n",
    "    j = np.argmax(model.predict(np.array(x)[np.newaxis]))-1\n",
    "    print(j, cleaned_tweets_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
