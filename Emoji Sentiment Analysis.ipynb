{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_full_text(tweet):\n",
    "    if 'extended_tweet' in tweet:\n",
    "        return tweet['extended_tweet']['full_text']\n",
    "    else:\n",
    "        return tweet['text']\n",
    "    \n",
    "def clean_up_text(text):\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',' ')\n",
    "    return text.strip()\n",
    "        \n",
    "def get_orig_text(tweet):\n",
    "    if 'retweeted_status' in tweet:\n",
    "        return get_orig_text(tweet['retweeted_status'])\n",
    "    else:\n",
    "        return clean_up_text(get_full_text(tweet))\n",
    "    \n",
    "directory = [f for f in glob.iglob('Downloads/BackLAOut/*')]\n",
    "i = 0\n",
    "raw_tweets = []\n",
    "for filepath in directory:\n",
    "    if len(raw_tweets) > 2e5: break # comment out when running full dataset\n",
    "\n",
    "    if (i > 0) and (i % (len(directory)//10) == 0):\n",
    "        print(str(i)+' of '+str(len(directory))+' files read, '+\n",
    "              str(len(raw_tweets))+' total tweets')\n",
    "    file = open(filepath, 'r')\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        text = get_orig_text(tweet)\n",
    "        if ('http://' not in text) and ('https://' not in text):\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            raw_tweets.append(text)\n",
    "    file.close()\n",
    "    i += 1\n",
    "raw_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = list(set(raw_tweets))\n",
    "joined_tweets = ' '.join(raw_tweets)\n",
    "\n",
    "len(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "raw_word_counts = Counter()\n",
    "raw_word_list = joined_tweets.split(' ')\n",
    "for word in raw_word_list:\n",
    "    raw_word_counts[word] += 1\n",
    "    \n",
    "def display_top_10_counts(d):\n",
    "    odct = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    for i, k in enumerate(list(odct)):\n",
    "        if i > 9:\n",
    "            del odct[k]\n",
    "    return odct\n",
    "\n",
    "display_top_10_counts(raw_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_char_counts = Counter()\n",
    "raw_char_list = [char for char in joined_tweets]\n",
    "for char in raw_char_list:\n",
    "    raw_char_counts[char] += 1\n",
    "\n",
    "display_top_10_counts(raw_char_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "def split_train_valid_test(X,y=[]):\n",
    "    percent_train = 0.9\n",
    "    test_size = 50\n",
    "\n",
    "    train_size = batch_size*int(percent_train*len(X)/batch_size)\n",
    "    valid_size = len(X) - test_size - train_size\n",
    "    assert(valid_size > 0)\n",
    "\n",
    "    X_valid, y_valid = X[:valid_size], y[:valid_size]\n",
    "    X_train, y_train = X[valid_size:-test_size], y[valid_size:-test_size]\n",
    "    X_test, y_test = X[-test_size:], y[-test_size:]\n",
    "#     assert((len(X_train)==len(y_train)) and\n",
    "#            (len(X_valid)==len(y_valid)) and\n",
    "#            (len(X_test)==len(y_test)))\n",
    "    print(len(X_train)/len(X),len(X_valid)/len(X),len(X_test)/len(X),len(X_train)/batch_size)\n",
    "    if y==[]:\n",
    "        return X_train,X_valid,X_test\n",
    "    else:\n",
    "        return X_train,X_valid,X_test,y_train,y_valid,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_train,raw_tweets_valid,raw_tweets_test = split_train_valid_test(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sentiment = {'☹': -1,\n",
    "                   '☺':  1,\n",
    "                   '♥':  1,\n",
    "                   '❣':  1,\n",
    "                   '❤':  1,\n",
    "                   '🐱':  1,\n",
    "                   '🐵':  0,\n",
    "                   '👅':  1,\n",
    "                   '👺': -1,\n",
    "                   '👿': -1,\n",
    "                   '💑':  1,\n",
    "                   '💓':  1,\n",
    "                   '💔': -1,\n",
    "                   '💕':  1,\n",
    "                   '💖':  1,\n",
    "                   '💗':  1,\n",
    "                   '💘':  1,\n",
    "                   '💙': -1,\n",
    "                   '💚':  1,\n",
    "                   '💛':  1,\n",
    "                   '💜':  1,\n",
    "                   '💝':  1,\n",
    "                   '💞':  1,\n",
    "                   '💟':  1,\n",
    "                   '🖤': -1,\n",
    "                   '😀':  1,\n",
    "                   '😁':  0,\n",
    "                   '😂':  0,\n",
    "                   '😃':  1,\n",
    "                   '😄':  1,\n",
    "                   '😅':  0,\n",
    "                   '😆':  0,\n",
    "                   '😇':  1,\n",
    "                   '😈':  1,\n",
    "                   '😉':  1,\n",
    "                   '😊':  1,\n",
    "                   '😋':  1,\n",
    "                   '😌':  1,\n",
    "                   '😍':  1,\n",
    "                   '😎':  1,\n",
    "                   '😏':  1,\n",
    "                   '😐':  0,\n",
    "                   '😑': -1,\n",
    "                   '😒':  0,\n",
    "                   '😓': -1,\n",
    "                   '😔': -1,\n",
    "                   '😕': -1,\n",
    "                   '😖': -1,\n",
    "                   '😗':  1,\n",
    "                   '😘':  1,\n",
    "                   '😙':  1,\n",
    "                   '😚':  1,\n",
    "                   '😛':  1,\n",
    "                   '😜':  1,\n",
    "                   '😝':  1,\n",
    "                   '😞': -1,\n",
    "                   '😟': -1,\n",
    "                   '😠': -1,\n",
    "                   '😡': -1,\n",
    "                   '😢': -1,\n",
    "                   '😣': -1,\n",
    "                   '😤': -1,\n",
    "                   '😥': -1,\n",
    "                   '😦': -1,\n",
    "                   '😧': -1,\n",
    "                   '😨': -1,\n",
    "                   '😩': -1,\n",
    "                   '😪':  0,\n",
    "                   '😫': -1,\n",
    "                   '😬':  0,\n",
    "                   '😭': -1,\n",
    "                   '😮':  0,\n",
    "                   '😯':  0,\n",
    "                   '😰': -1,\n",
    "                   '😱':  0,\n",
    "                   '😲': -1,\n",
    "                   '😳': -1,\n",
    "                   '😴':  0,\n",
    "                   '😵': -1,\n",
    "                   '😶':  0,\n",
    "                   '😷':  0,\n",
    "                   '😸':  1,\n",
    "                   '😹':  0,\n",
    "                   '😺':  1,\n",
    "                   '😻':  1,\n",
    "                   '😼':  1,\n",
    "                   '😽':  1,\n",
    "                   '😾': -1,\n",
    "                   '😿': -1,\n",
    "                   '🙀':  0,\n",
    "                   '🙁': -1,\n",
    "                   '🙂':  1,\n",
    "                   '🙃':  1,\n",
    "                   '🙄':  0,\n",
    "                   '🙈':  0,\n",
    "                   '🙉':  0,\n",
    "                   '🙊':  0,\n",
    "                   '🤍':  1,\n",
    "                   '🤎':  1,\n",
    "                   '🤐':  0,\n",
    "                   '🤑':  1,\n",
    "                   '🤒': -1,\n",
    "                   '🤓':  0,\n",
    "                   '🤔':  0,\n",
    "                   '🤕': -1,\n",
    "                   '🤖':  0,\n",
    "                   '🤗':  1,\n",
    "                   '🤛':  1,\n",
    "                   '🤛🏻':  1,\n",
    "                   '🤛🏼':  1,\n",
    "                   '🤛🏽':  1,\n",
    "                   '🤛🏾':  1,\n",
    "                   '🤛🏿':  1,\n",
    "                   '🤜':  1,\n",
    "                   '🤜🏻':  1,\n",
    "                   '🤜🏼':  1,\n",
    "                   '🤜🏽':  1,\n",
    "                   '🤜🏾':  1,\n",
    "                   '🤜🏿':  1,\n",
    "                   '🤠':  1,\n",
    "                   '🤡':  1,\n",
    "                   '🤢': -1,\n",
    "                   '🤣':  0,\n",
    "                   '🤤':  1,\n",
    "                   '🤥': -1,\n",
    "                   '🤦': -1,\n",
    "                   '🤦🏻': -1,\n",
    "                   '🤦🏼': -1,\n",
    "                   '🤦🏽': -1,\n",
    "                   '🤦🏾': -1,\n",
    "                   '🤦🏿': -1,\n",
    "                   '🤧':  0,\n",
    "                   '🤨':  0,\n",
    "                   '🤪':  0,\n",
    "                   '🤫':  0,\n",
    "                   '🤬': -1,\n",
    "                   '🤭':  1,\n",
    "                   '🤮': -1,\n",
    "                   '🤯':  0,\n",
    "                   '🥰':  1,\n",
    "                   '🥱':  0,\n",
    "                   '🥳':  1,\n",
    "                   '🥴':  1,\n",
    "                   '🥵': -1,\n",
    "                   '🥶': -1,\n",
    "                   '🥺': -1,\n",
    "                   '🧐':  0,\n",
    "                   '🧡':  1}\n",
    "def clean_tweets(X):\n",
    "    cleaned_tweets = []\n",
    "    i = 0\n",
    "    for i, origtweet in enumerate(X):\n",
    "        i += 1\n",
    "        if i % (len(X)//10) == 0:\n",
    "            print(str(10*i/(len(X)//10))+'% read')\n",
    "\n",
    "        emojis = []\n",
    "        tweet = ''\n",
    "        no_emojis = True\n",
    "        for e in emoji_sentiment:\n",
    "            if e in origtweet:\n",
    "                no_emojis = False\n",
    "                for char in origtweet:\n",
    "                    if char == e:\n",
    "                        tweet += ' '+char+' ' # this is so we can parse out the emoji\n",
    "\n",
    "                        emojis.append(e)\n",
    "                    else:\n",
    "                        tweet += char\n",
    "        if no_emojis:\n",
    "            continue\n",
    "\n",
    "        tweet_cleaned = []\n",
    "        for word in clean_up_text(tweet).split(' '):\n",
    "            if word[0] != '@':\n",
    "                tweet_cleaned.append(word)\n",
    "        tweet_cleaned = ' '.join(tweet_cleaned)\n",
    "\n",
    "        cleaned_tweets.append(tweet_cleaned)\n",
    "        \n",
    "    return list(set(cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets_train = clean_tweets(raw_tweets_train)\n",
    "cleaned_tweets_valid = clean_tweets(raw_tweets_valid)\n",
    "cleaned_tweets_test = clean_tweets(raw_tweets_test)\n",
    "\n",
    "len(cleaned_tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get__X_and__y(cleaned_tweets):\n",
    "    _X = []\n",
    "    _y = []\n",
    "    not_emojis = 0\n",
    "    for i, tweet in enumerate(cleaned_tweets):\n",
    "        sentiments = []\n",
    "        not_emoji = False\n",
    "        for e in emoji_sentiment:\n",
    "            if 'not '+e in tweet:\n",
    "                not_emoji = True\n",
    "        if not_emoji:\n",
    "            not_emojis += 1\n",
    "            continue\n",
    "        emojis = []\n",
    "        for word in tweet.split(' '):\n",
    "            for e in emoji_sentiment:\n",
    "                if word == e:\n",
    "                    sentiments.append(emoji_sentiment[e])\n",
    "                    emojis.append(e)\n",
    "        # only include tweets that have exactly one of the selected emojis\n",
    "        if len(set(sentiments)) == 1:\n",
    "            for e in emojis:\n",
    "                tweet = tweet.replace(e,'')\n",
    "            _X.append(clean_up_text(tweet))\n",
    "            _y.append(sentiments[0])\n",
    "    return _X, _y, not_emojis\n",
    "_X_train, _y_train, not_emojis_train = get__X_and__y(cleaned_tweets_train)\n",
    "_X_valid, _y_valid, not_emojis_valid = get__X_and__y(cleaned_tweets_valid)\n",
    "_X_test, _y_test, not_emojis_test = get__X_and__y(cleaned_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def get_most_frequent_keys(d,n):\n",
    "#     a = np.array(list(d.values()))\n",
    "#     thresh = min(a[np.argpartition(a,-n)][-n:])\n",
    "#     inds = a > 0.9 * thresh\n",
    "#     d = dict(np.array(list(d.items()))[inds])\n",
    "#     for k in d:\n",
    "#         d[k] = int(d[k])\n",
    "#     odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "#     return odct\n",
    "\n",
    "# n_classes = 10\n",
    "# most_frequent_emojis_train = get_most_frequent_keys(emoji_counts_train, n_classes)\n",
    "# most_frequent_emojis_valid = get_most_frequent_keys(emoji_counts_valid, n_classes)\n",
    "# most_frequent_emojis_test = get_most_frequent_keys(emoji_counts_test, n_classes)\n",
    "# n_classes = len(most_frequent_emojis_train)\n",
    "# most_frequent_emojis_train,len(most_frequent_emojis_train)\n",
    "\n",
    "emoji_counts = {-1:0,0:0,1:0}\n",
    "\n",
    "for i in _y_train:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "words_list = ' '.join(_X_train+_X_valid+_X_test).split(' ')\n",
    "word_counts = Counter()\n",
    "for word in words_list:\n",
    "    word_counts[word] += 1\n",
    "    \n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    return to_categorical(_y, num_classes=3)\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "X_train = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_train]\n",
    "X_valid = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_valid]\n",
    "X_test = [[word2id[word] for word in tweet.split(' ')] for tweet in _X_test]\n",
    "\n",
    "max_words = 140\n",
    "X_train= sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "y_train = convert_to_one_hot(_y_train)\n",
    "y_valid = convert_to_one_hot(_y_valid)\n",
    "y_test = convert_to_one_hot(_y_test)\n",
    "\n",
    "X_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.arange(3)-1, y=_y_train)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "def calc_n_neurons():\n",
    "    alpha = np.log(len(X_train))\n",
    "    Ni = model.layers[-1].input.shape[-1]\n",
    "    No = model.layers[-1].output.shape[-1]\n",
    "    n_neurons = max(int(2 * len(X_train) / (alpha * (Ni + No))), 1)\n",
    "    print(alpha,Ni,No,n_neurons)\n",
    "    return n_neurons\n",
    "\n",
    "model.add(LSTM(calc_n_neurons(), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(calc_n_neurons()))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 25\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for word,i in word2id.items()}\n",
    "\n",
    "model.load_weights(filepath)\n",
    "for i,x in enumerate(X_test):\n",
    "    j = np.argmax(model.predict(np.array(x)[np.newaxis]))-1\n",
    "    print(j, cleaned_tweets_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
