{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_full_text(tweet):\n",
    "    if 'extended_tweet' in tweet:\n",
    "        return tweet['extended_tweet']['full_text']\n",
    "    else:\n",
    "        return tweet['text']\n",
    "    \n",
    "def clean_up_text(text):\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',' ')\n",
    "    return text.strip()\n",
    "        \n",
    "def get_orig_text(tweet):\n",
    "    if 'retweeted_status' in tweet:\n",
    "        return get_orig_text(tweet['retweeted_status'])\n",
    "    else:\n",
    "        return clean_up_text(get_full_text(tweet))\n",
    "    \n",
    "directory = [f for f in glob.iglob('Downloads/BackLAOut/*')]\n",
    "i = 0\n",
    "raw_tweets = []\n",
    "for filepath in directory:\n",
    "    if (i > 0) and (i % (len(directory)//10) == 0):\n",
    "        print(str(i)+' of '+str(len(directory))+' files read, '+\n",
    "              str(len(raw_tweets))+' total tweets')\n",
    "    file = open(filepath, 'r')\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        text = get_orig_text(tweet)\n",
    "        if ('http://' not in text) and ('https://' not in text):\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            raw_tweets.append(text)\n",
    "    file.close()\n",
    "    i += 1\n",
    "raw_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = list(set(raw_tweets))\n",
    "joined_tweets = ' '.join(raw_tweets)\n",
    "\n",
    "len(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "raw_word_counts = Counter()\n",
    "raw_word_list = joined_tweets.split(' ')\n",
    "for word in raw_word_list:\n",
    "    raw_word_counts[word] += 1\n",
    "    \n",
    "def display_top_10_counts(d):\n",
    "    odct = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    for i, k in enumerate(list(odct)):\n",
    "        if i > 9:\n",
    "            del odct[k]\n",
    "    return odct\n",
    "\n",
    "display_top_10_counts(raw_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_char_counts = Counter()\n",
    "raw_char_list = [char for char in joined_tweets]\n",
    "for char in raw_char_list:\n",
    "    raw_char_counts[char] += 1\n",
    "\n",
    "display_top_10_counts(raw_char_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_emoji_counts = {}\n",
    "for char in raw_char_counts:\n",
    "    if len(char) == 1:\n",
    "        if ord(char) > 99999:\n",
    "            raw_emoji_counts[char] = raw_char_counts[char]\n",
    "\n",
    "display_top_10_counts(raw_emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "emoji_counts = Counter()\n",
    "i = 0\n",
    "for i, origtweet in enumerate(raw_tweets):\n",
    "    i += 1\n",
    "    if i % (len(raw_tweets)//10) == 0:\n",
    "        print(str(10*i/(len(raw_tweets)//10))+'% read')\n",
    "        \n",
    "    emojis = []\n",
    "    tweet = ''\n",
    "    no_emojis = True\n",
    "    for emoji in raw_emoji_counts:\n",
    "        if emoji in origtweet:\n",
    "            no_emojis = False\n",
    "            for char in origtweet:\n",
    "                if char == emoji:\n",
    "                    tweet += ' '+char+' ' # this is so we can parse out the emoji\n",
    "\n",
    "                    emojis.append(emoji)\n",
    "                else:\n",
    "                    tweet += char\n",
    "    if no_emojis:\n",
    "        continue\n",
    "                \n",
    "    tweet_cleaned = []\n",
    "    for word in clean_up_text(tweet).split(' '):\n",
    "        if word[0] != '@':\n",
    "            tweet_cleaned.append(word)\n",
    "    tweet_cleaned = ' '.join(tweet_cleaned)\n",
    "    \n",
    "    cleaned_tweets.append(tweet_cleaned)\n",
    "    \n",
    "    for emoji in set(emojis):\n",
    "        emoji_counts[emoji] += 1\n",
    "        \n",
    "cleaned_tweets = list(set(cleaned_tweets))\n",
    "\n",
    "len(cleaned_tweets), display_top_10_counts(emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_most_frequent_keys(d,n):\n",
    "    a = np.array(list(d.values()))\n",
    "    thresh = min(a[np.argpartition(a,-n)][-n:])\n",
    "    inds = a > 0.9 * thresh\n",
    "    d = dict(np.array(list(d.items()))[inds])\n",
    "    for k in d:\n",
    "        d[k] = 1 / int(d[k])\n",
    "    return d\n",
    "\n",
    "n_classes = 10\n",
    "most_frequent_emojis = get_most_frequent_keys(emoji_counts, n_classes)\n",
    "n_classes = len(most_frequent_emojis)\n",
    "most_frequent_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tweets = []\n",
    "_X = []\n",
    "_y = []\n",
    "not_emojis = 0\n",
    "for i, tweet in enumerate(cleaned_tweets):\n",
    "    emojis = []\n",
    "    not_emoji = False\n",
    "    for emoji in most_frequent_emojis:\n",
    "        if 'not '+emoji in tweet:\n",
    "            not_emoji = True\n",
    "    if not_emoji:\n",
    "        not_emojis += 1\n",
    "        continue\n",
    "    for word in tweet.split(' '):\n",
    "        for emoji in most_frequent_emojis:\n",
    "            if word == emoji:\n",
    "                emojis.append(emoji)\n",
    "    # only include tweets that have exactly one of the selected emojis\n",
    "    if len(set(emojis)) == 1:\n",
    "        reduced_tweets.append(tweet)\n",
    "        tweet_stripped = clean_up_text(tweet.replace(emojis[0],'')) # strip out the emoji\n",
    "        _X.append(tweet_stripped)\n",
    "        _y.append(emojis)\n",
    "not_emojis, reduced_tweets[:5], _X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "words_list = ' '.join(_X).split(' ')\n",
    "word_counts = Counter()\n",
    "for word in words_list:\n",
    "    word_counts[word] += 1\n",
    "    \n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    return to_categorical(_y, num_classes=n_classes)\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "X = [[word2id[word] for word in tweet.split(' ')] for tweet in _X]\n",
    "\n",
    "max_words = 140\n",
    "X = sequence.pad_sequences(X, maxlen=max_words)\n",
    "\n",
    "emoji2id = {}\n",
    "for i,emoji in enumerate(most_frequent_emojis):\n",
    "    emoji2id[emoji] = i\n",
    "    \n",
    "y = convert_to_one_hot([[emoji2id[emoji[0]]] for emoji in _y])\n",
    "\n",
    "word2id[''], X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_size=100\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percent_validation = 0.2\n",
    "batch_size = 64\n",
    "test_size = len(X)//3\n",
    "tv_size = len(X) - test_size\n",
    "v_size = tv_size-batch_size*int((1-percent_validation)*tv_size/batch_size)\n",
    "X_valid, y_valid = X[:v_size], y[:v_size]\n",
    "X_train, y_train = X[v_size:tv_size], y[v_size:tv_size]\n",
    "X_test, y_test = X[tv_size:], y[tv_size:]\n",
    "assert((len(X_train)==len(y_train)) and\n",
    "       (len(X_valid)==len(y_valid)) and\n",
    "       (len(X_test)==len(y_test)))\n",
    "len(X_train)/len(X),len(X_valid)/len(X),len(X_test)/len(X),len(X_train)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = np.array([[emoji2id[k],v] for k,v in most_frequent_emojis.items()])\n",
    "class_weight[:,1] /= np.exp(np.mean(np.log(class_weight[:,1])))\n",
    "class_weight = dict(class_weight)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 50\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weight, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2emoji = {i:emoji for emoji,i in emoji2id.items()}\n",
    "id2word = {i:word for word,i in word2id.items()}\n",
    "\n",
    "model.load_weights(filepath)\n",
    "for i in range(test_size):\n",
    "    j = np.argmax(model.predict(np.array(X[-i-1])[np.newaxis]))\n",
    "    print(id2emoji[j], reduced_tweets[-i-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
