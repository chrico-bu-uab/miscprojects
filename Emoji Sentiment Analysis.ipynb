{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_tweets import get_raw_tweets\n",
    "import pickle\n",
    "\n",
    "raw_tweets = get_raw_tweets('BackLAOut',5e7)\n",
    "raw_tweets += get_raw_tweets('BackNYOut',5e7)\n",
    "\n",
    "with open('raw_BackLA_BackNY', 'wb') as fp:\n",
    "    pickle.dump(raw_tweets, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_tweets import clean_tweets\n",
    "import pickle\n",
    "with open('raw_BackLA_BackNY', 'rb') as fp:\n",
    "    raw_tweets = [tweet for tweet in pickle.load(fp)]\n",
    "    \n",
    "cleaned_tweets = clean_tweets(raw_tweets)\n",
    "\n",
    "with open('clean_BackLA_BackNY', 'wb') as fp:\n",
    "    pickle.dump(cleaned_tweets, fp)\n",
    "    \n",
    "len(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import emoji\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from extract_tweets import get__X_and__y\n",
    "import pickle\n",
    "with open('clean_BackLA_BackNY', 'rb') as fp:\n",
    "    cleaned_tweets = [tweet for tweet in pickle.load(fp)]\n",
    "\n",
    "random.shuffle(cleaned_tweets)\n",
    "\n",
    "emojis = emoji.core.unicode_codes.UNICODE_EMOJI\n",
    "emoji_sentiment1 = dict(zip(emojis, [0]*len(emojis)))\n",
    "emoji_sentiment2 = \\\n",
    "OrderedDict([('â˜¹', -1),('ðŸ‘º', -1),('ðŸ‘¿', -1),('ðŸ’”', -1),('ðŸ’™', -1),('ðŸ–¤', -1),('ðŸ˜‘', -1),\n",
    "             ('ðŸ˜’', -1),('ðŸ˜“', -1),('ðŸ˜”', -1),('ðŸ˜•', -1),('ðŸ˜–', -1),('ðŸ˜ž', -1),('ðŸ˜Ÿ', -1),\n",
    "             ('ðŸ˜ ', -1),('ðŸ˜¡', -1),('ðŸ˜¢', -1),('ðŸ˜£', -1),('ðŸ˜¤', -1),('ðŸ˜¥', -1),('ðŸ˜¦', -1),\n",
    "             ('ðŸ˜§', -1),('ðŸ˜¨', -1),('ðŸ˜©', -1),('ðŸ˜«', -1),('ðŸ˜­', -1),('ðŸ˜°', -1),('ðŸ˜±', -1),\n",
    "             ('ðŸ˜²', -1),('ðŸ˜³', -1),('ðŸ˜µ', -1),('ðŸ˜¾', -1),('ðŸ˜¿', -1),('ðŸ™', -1),('ðŸ™„', -1),\n",
    "             ('ðŸ¤’', -1),('ðŸ¤•', -1),('ðŸ¤¢', -1),('ðŸ¤¥', -1),('ðŸ¤¬', -1),('ðŸ¤®', -1),('ðŸ¥µ', -1),\n",
    "             ('ðŸ¥º', -1),('ðŸ¤¦', -1),('ðŸ¤¦ðŸ»', -1),('ðŸ¤¦ðŸ¼', -1),('ðŸ¤¦ðŸ½', -1),('ðŸ¤¦ðŸ¾', -1),('ðŸ¤¦ðŸ¿', -1),\n",
    "             \n",
    "             ('â˜º',  1),('â™¥',  1),('â£',  1),('â¤',  1),('ðŸ±',  1),('ðŸ‘…',  1),('ðŸ’‘',  1),\n",
    "             ('ðŸ’“',  1),('ðŸ’•',  1),('ðŸ’–',  1),('ðŸ’—',  1),('ðŸ’˜',  1),('ðŸ’š',  1),('ðŸ’›',  1),\n",
    "             ('ðŸ’œ',  1),('ðŸ’',  1),('ðŸ’ž',  1),('ðŸ’Ÿ',  1),('ðŸ˜€',  1),('ðŸ˜',  1),('ðŸ˜‚',  1),\n",
    "             ('ðŸ˜ƒ',  1),('ðŸ˜„',  1),('ðŸ˜…',  1),('ðŸ˜†',  1),('ðŸ˜‡',  1),('ðŸ˜ˆ',  1),('ðŸ˜‰',  1),\n",
    "             ('ðŸ˜Š',  1),('ðŸ˜‹',  1),('ðŸ˜Œ',  1),('ðŸ˜',  1),('ðŸ˜Ž',  1),('ðŸ˜',  1),('ðŸ˜—',  1),\n",
    "             ('ðŸ˜˜',  1),('ðŸ˜™',  1),('ðŸ˜š',  1),('ðŸ˜›',  1),('ðŸ˜œ',  1),('ðŸ˜',  1),('ðŸ˜¸',  1),\n",
    "             ('ðŸ˜º',  1),('ðŸ˜»',  1),('ðŸ˜¼',  1),('ðŸ˜½',  1),('ðŸ™‚',  1),('ðŸ™ƒ',  1),('ðŸ¤',  1),\n",
    "             ('ðŸ¤Ž',  1),('ðŸ¤‘',  1),('ðŸ¤—',  1),('ðŸ¤›',  1),('ðŸ¤›ðŸ»',  1),('ðŸ¤›ðŸ¼',  1),('ðŸ¤›ðŸ½',  1),\n",
    "             ('ðŸ¤›ðŸ¾',  1),('ðŸ¤›ðŸ¿',  1),('ðŸ¤œ',  1),('ðŸ¤œðŸ»',  1),('ðŸ¤œðŸ¼',  1),('ðŸ¤œðŸ½',  1),('ðŸ¤œðŸ¾',  1),\n",
    "             ('ðŸ¤œðŸ¿',  1),('ðŸ¤ ',  1),('ðŸ¤¡',  1),('ðŸ¤¤',  1),('ðŸ¤ª',  1),('ðŸ¤­',  1),('ðŸ¥°',  1),\n",
    "             ('ðŸ¥³',  1),('ðŸ¥´',  1),('ðŸ§¡',  1)])\n",
    "for e in emojis:\n",
    "    if e in emoji_sentiment2:\n",
    "        del emoji_sentiment1[e]\n",
    "emoji_sentiment = OrderedDict(chain(emoji_sentiment1.items(),emoji_sentiment2.items()))\n",
    "\n",
    "with open('emoji_sentiment', 'wb') as fp:\n",
    "    pickle.dump(emoji_sentiment, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_tweets import get__X_and__y\n",
    "import pickle\n",
    "with open('emoji_sentiment', 'rb') as fp:\n",
    "    emoji_sentiment = pickle.load(fp)\n",
    "\n",
    "_X, _y = get__X_and__y(cleaned_tweets, emoji_sentiment)\n",
    "\n",
    "with open('_X', 'wb') as fp:\n",
    "    pickle.dump(_X, fp)\n",
    "with open('_y', 'wb') as fp:\n",
    "    pickle.dump(_y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import pickle\n",
    "with open('_X', 'rb') as fp:\n",
    "    _X = [tweet for tweet in pickle.load(fp)]\n",
    "with open('_y', 'rb') as fp:\n",
    "    _y = pickle.load(fp)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def split_tr_val_te(X):\n",
    "    percent_tr = 1/3\n",
    "    percent_val = 1/3\n",
    "\n",
    "    tr_size = batch_size*ceil(percent_tr*len(X)/batch_size)\n",
    "    val_size = ceil(percent_val*(len(X)-tr_size)/(1-percent_tr))\n",
    "    te_size = len(X) - tr_size - val_size\n",
    "\n",
    "    X_val = X[:val_size]\n",
    "    X_tr = X[val_size:-te_size]\n",
    "    X_te = X[-te_size:]\n",
    "    print(len(X),len(X_tr)/len(X),len(X_val)/len(X),len(X_te)/len(X),len(X_tr)/batch_size)\n",
    "    return X_tr, X_val, X_te\n",
    "\n",
    "_X_tr,_X_val,_X_te = split_tr_val_te(_X)\n",
    "_y_tr,_y_val,_y_te = split_tr_val_te(_y)\n",
    "\n",
    "def split_tweets_into_words(_X):\n",
    "    return [[word for word in tweet.split(' ')] for tweet in _X]\n",
    "\n",
    "_X_tr = split_tweets_into_words(_X_tr)\n",
    "_X_val = split_tweets_into_words(_X_val)\n",
    "_X_te = split_tweets_into_words(_X_te)\n",
    "\n",
    "emoji_counts = Counter()\n",
    "for i in _y_tr:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts, {k:v/sum(emoji_counts.values()) for k,v in emoji_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in _X_tr]+\n",
    "                         [' '.join(tweet) for tweet in _X_val]+\n",
    "                         [' '.join(tweet) for tweet in _X_te]).split(' ')\n",
    "\n",
    "words = np.unique(runonsentence)\n",
    "def get_counts_adv(_X):\n",
    "    d1 = dict(zip(words,[0]*len(words)))\n",
    "    d2 = dict(zip(words,[0]*len(words)))\n",
    "    d3 = dict(zip(words,[0]*len(words)))\n",
    "    \n",
    "    for x in _X:\n",
    "        for w in x:\n",
    "            d1[w] += 1\n",
    "    m1 = np.mean(list(d1.values()))\n",
    "    sd1 = np.std(list(d1.values()))\n",
    "    print(m1,sd1)\n",
    "    \n",
    "    for x in _X:\n",
    "        d = Counter()\n",
    "        for w in x:\n",
    "            d[w] = 1\n",
    "        for w in d:\n",
    "            d2[w] += d[w]\n",
    "    m2 = np.mean(list(d2.values()))\n",
    "    sd2 = np.std(list(d2.values()))\n",
    "    print(m2,sd2)\n",
    "    \n",
    "    fdfdsf = list(reduce(lambda i, j: set(i) | set(j), _X))\n",
    "    for w in fdfdsf:\n",
    "        d3[w] = (d1[w] - m1) / sd1 + (d2[w] - m2) / sd2\n",
    "\n",
    "    return d3\n",
    "\n",
    "wc_tr = get_counts_adv(_X_tr)\n",
    "wc_val = get_counts_adv(_X_val)\n",
    "wc_te = get_counts_adv(_X_te)\n",
    "\n",
    "for w in words:\n",
    "    if wc_tr[w]*wc_val[w]*wc_te[w]==0:\n",
    "        del wc_tr[w]\n",
    "        del wc_val[w]\n",
    "        del wc_te[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_keys(d,n):\n",
    "    a = np.array(list(d.values()),dtype='float')\n",
    "    a -= np.nanmin(a)\n",
    "    N = min(n,len(d))\n",
    "    inds = a >= 0.99 * min(a[np.argpartition(a,-N)][-N:])\n",
    "    d = dict(np.array(list(d.items()))[inds])\n",
    "    for k in d:\n",
    "        d[k] = float(d[k])\n",
    "    odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "    return odct\n",
    "\n",
    "vocab = get_most_frequent_keys(wc_tr,5000)\n",
    "print(len(vocab))\n",
    "\n",
    "def get_counts(i):\n",
    "    d = Counter()\n",
    "    for j in i:\n",
    "        d[j] += 1\n",
    "    return d\n",
    "\n",
    "runonsentence = [w if w in vocab else '<UNK>' for w in runonsentence]\n",
    "word_counts = get_counts(runonsentence)\n",
    "    \n",
    "assert(min(list(word_counts.values())) > 1)\n",
    "\n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size, word_counts['<UNK>'], _X_tr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([(0,len(x)) for x in _X_tr]+\n",
    "           [(1,len(x)) for x in _X_val]+\n",
    "           [(2,len(x)) for x in _X_te])\n",
    "T=np.vstack([T.T,np.arange(len(T))]).T\n",
    "tr=len(_X_tr)\n",
    "val=len(_X_val)\n",
    "T[:,2][tr:] -= tr\n",
    "T[:,2][tr+val:] -= val\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2],T[0:2],T[tr:tr+2],T[tr+val:tr+val+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for t in T:\n",
    "    for j in range(t[1]):\n",
    "        if t[0] == 0:\n",
    "            _X_tr[t[2]][j] = runonsentence[i]\n",
    "        elif t[0] == 1:\n",
    "            _X_val[t[2]][j] = runonsentence[i]\n",
    "        else:\n",
    "            _X_te[t[2]][j] = runonsentence[i]\n",
    "        i += 1\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "with open('emoji_sentiment', 'rb') as fp:\n",
    "    emoji_sentiment = pickle.load(fp)\n",
    "\n",
    "def convert_to_id(_X):\n",
    "    return [[word2id[word] for word in tweet] for tweet in _X]\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    _y = np.array(_y) - np.array(_y).min()\n",
    "    _y = _y * (len(np.unique(_y))-1) / max(_y)\n",
    "    print(_y)\n",
    "    return to_categorical(_y, num_classes=len(np.unique(list(emoji_sentiment.values()))))\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "max_words = max([len(x) for x in _X])\n",
    "X_tr= sequence.pad_sequences(convert_to_id(_X_tr), maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(convert_to_id(_X_val), maxlen=max_words)\n",
    "X_te = sequence.pad_sequences(convert_to_id(_X_te), maxlen=max_words)\n",
    "\n",
    "y_tr = convert_to_one_hot(_y_tr)\n",
    "y_val = convert_to_one_hot(_y_val)\n",
    "y_te = convert_to_one_hot(_y_te)\n",
    "\n",
    "X_tr[:20], y_tr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.arange(len(y_tr[0])),\n",
    "                                                  y=np.argmax(y_tr,axis=1))\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                   classes=np.arange(len(y_tr[0]))-1,\n",
    "#                                                   y=_y_tr)\n",
    "\n",
    "print(np.sum(y_tr,axis=0)*class_weights)\n",
    "\n",
    "class_weights = {i:w for i,w in zip(range(len(y_tr[0])),class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "embedding_size=32\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "n_neurons = ceil(np.sqrt(0.15 * len(X_tr)))\n",
    "model.add(Bidirectional(LSTM(n_neurons, return_sequences=True),\n",
    "                        backward_layer=LSTM(n_neurons, return_sequences=True,\n",
    "                                            activation='relu', go_backwards=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(n_neurons))\n",
    "model.add(Dropout(0.2))\n",
    "if len(class_weights) == 2:\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "else:\n",
    "    model.add(Dense(len(class_weights), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy' if len(class_weights) == 2 else 'categorical_crossentropy'\n",
    "model.compile(loss=loss, optimizer='Nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 25\n",
    "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "yhat_te = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=np.argmax(yhat_te,axis=1)==np.argmax(y_te,axis=1)\n",
    "yhat_te,y_te,score,np.mean(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
