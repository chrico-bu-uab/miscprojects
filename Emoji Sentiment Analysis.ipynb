{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_full_text(tweet):\n",
    "    if 'extended_tweet' in tweet:\n",
    "        return tweet['extended_tweet']['full_text']\n",
    "    else:\n",
    "        return tweet['text']\n",
    "    \n",
    "def clean_up_text(text):\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',' ')\n",
    "    return text.strip()\n",
    "        \n",
    "def get_orig_text(tweet):\n",
    "    if 'retweeted_status' in tweet:\n",
    "        return get_orig_text(tweet['retweeted_status'])\n",
    "    else:\n",
    "        return clean_up_text(get_full_text(tweet))\n",
    "    \n",
    "def get_raw_tweets(foldername,stop):\n",
    "    directory=[f for f in glob.iglob('Downloads/'+foldername+'/*')]\n",
    "    raw_tweets = []\n",
    "    i = 0\n",
    "    msg = True\n",
    "    for filepath in directory:\n",
    "        file = open(filepath, 'r')\n",
    "        for line in file:\n",
    "            \n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            text = get_orig_text(tweet)\n",
    "            \n",
    "            if ('http://' not in text) and ('https://' not in text):\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "                \n",
    "                raw_tweets.append(text)\n",
    "                msg = True\n",
    "                \n",
    "                if len(raw_tweets) >= stop:\n",
    "                    file.close()\n",
    "                    return list(set(raw_tweets))\n",
    "                \n",
    "                if (stop<float('inf')) and msg:\n",
    "                    if (len(raw_tweets) > 0) and (len(raw_tweets) % (stop//20) == 0):\n",
    "                        print(str(len(raw_tweets))+' of '+str(stop)+' tweets read')\n",
    "                    msg = False\n",
    "                \n",
    "        file.close()\n",
    "        \n",
    "        if stop==float('inf'):\n",
    "            if (i > 0) and (i % (len(directory)//20) == 0):\n",
    "                print(str(i)+' of '+str(len(directory))+' files read ('+\n",
    "                      str(len(raw_tweets))+' total tweets)')\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return list(set(raw_tweets))\n",
    "\n",
    "raw_tweets = get_raw_tweets('BackLAOut',float('inf'))\n",
    "raw_tweets += get_raw_tweets('BackNYOut',float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweets = ' '.join(raw_tweets)\n",
    "\n",
    "len(raw_tweets), raw_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def get_counts(i):\n",
    "    d = Counter()\n",
    "    for j in i:\n",
    "        d[j] += 1\n",
    "    return d\n",
    "\n",
    "raw_char_counts = get_counts(joined_tweets)\n",
    "raw_word_counts = get_counts(joined_tweets.split(' '))\n",
    "    \n",
    "def display_top_10_counts(d):\n",
    "    odct = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    for i, k in enumerate(list(odct)):\n",
    "        if i > 9:\n",
    "            del odct[k]\n",
    "    return odct\n",
    "\n",
    "display_top_10_counts(raw_char_counts), display_top_10_counts(raw_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sentiment = {'â˜¹': -1, 'â˜º':  1, 'â™¥':  1, 'â£':  1, 'â¤':  1, 'ðŸ±':  1, 'ðŸµ':  0, \n",
    "                   'ðŸ‘…':  1, 'ðŸ‘º': -1, 'ðŸ‘¿': -1, 'ðŸ’‘':  1, 'ðŸ’“':  1, 'ðŸ’”': -1, 'ðŸ’•':  1,\n",
    "                   'ðŸ’–':  1, 'ðŸ’—':  1, 'ðŸ’˜':  1, 'ðŸ’™': -1, 'ðŸ’š':  1, 'ðŸ’›':  1, 'ðŸ’œ':  1,\n",
    "                   'ðŸ’':  1, 'ðŸ’ž':  1, 'ðŸ’Ÿ':  1, 'ðŸ–¤': -1, 'ðŸ˜€':  1, 'ðŸ˜':  1, 'ðŸ˜‚':  1,\n",
    "                   'ðŸ˜ƒ':  1, 'ðŸ˜„':  1, 'ðŸ˜…':  1, 'ðŸ˜†':  1, 'ðŸ˜‡':  1, 'ðŸ˜ˆ':  1, 'ðŸ˜‰':  1,\n",
    "                   'ðŸ˜Š':  1, 'ðŸ˜‹':  1, 'ðŸ˜Œ':  1, 'ðŸ˜':  1, 'ðŸ˜Ž':  1, 'ðŸ˜':  1, 'ðŸ˜':  0,\n",
    "                   'ðŸ˜‘': -1, 'ðŸ˜’': -1, 'ðŸ˜“': -1, 'ðŸ˜”': -1, 'ðŸ˜•': -1, 'ðŸ˜–': -1, 'ðŸ˜—':  1,\n",
    "                   'ðŸ˜˜':  1, 'ðŸ˜™':  1, 'ðŸ˜š':  1, 'ðŸ˜›':  1, 'ðŸ˜œ':  1, 'ðŸ˜':  1, 'ðŸ˜ž': -1,\n",
    "                   'ðŸ˜Ÿ': -1, 'ðŸ˜ ': -1, 'ðŸ˜¡': -1, 'ðŸ˜¢': -1, 'ðŸ˜£': -1, 'ðŸ˜¤': -1, 'ðŸ˜¥': -1,\n",
    "                   'ðŸ˜¦': -1, 'ðŸ˜§': -1, 'ðŸ˜¨': -1, 'ðŸ˜©': -1, 'ðŸ˜ª':  0, 'ðŸ˜«': -1, 'ðŸ˜¬':  0,\n",
    "                   'ðŸ˜­': -1, 'ðŸ˜®':  0, 'ðŸ˜¯':  0, 'ðŸ˜°': -1, 'ðŸ˜±': -1, 'ðŸ˜²': -1, 'ðŸ˜³': -1,\n",
    "                   'ðŸ˜´':  0, 'ðŸ˜µ': -1, 'ðŸ˜¶':  0, 'ðŸ˜·':  0, 'ðŸ˜¸':  1, 'ðŸ˜¹':  0, 'ðŸ˜º':  1,\n",
    "                   'ðŸ˜»':  1, 'ðŸ˜¼':  1, 'ðŸ˜½':  1, 'ðŸ˜¾': -1, 'ðŸ˜¿': -1, 'ðŸ™€':  0, 'ðŸ™': -1,\n",
    "                   'ðŸ™‚':  1, 'ðŸ™ƒ':  1, 'ðŸ™„': -1, 'ðŸ™ˆ':  0, 'ðŸ™‰':  0, 'ðŸ™Š':  0, 'ðŸ¤':  1,\n",
    "                   'ðŸ¤Ž':  1, 'ðŸ¤':  0, 'ðŸ¤‘':  1, 'ðŸ¤’': -1, 'ðŸ¤“':  1, 'ðŸ¤”':  0, 'ðŸ¤•': -1,\n",
    "                   'ðŸ¤–':  0, 'ðŸ¤—':  1, 'ðŸ¤›':  1, 'ðŸ¤›ðŸ»':  1, 'ðŸ¤›ðŸ¼':  1, 'ðŸ¤›ðŸ½':  1, 'ðŸ¤›ðŸ¾':  1,\n",
    "                   'ðŸ¤›ðŸ¿':  1, 'ðŸ¤œ':  1, 'ðŸ¤œðŸ»':  1, 'ðŸ¤œðŸ¼':  1, 'ðŸ¤œðŸ½':  1, 'ðŸ¤œðŸ¾':  1, 'ðŸ¤œðŸ¿':  1,\n",
    "                   'ðŸ¤ ':  1, 'ðŸ¤¡':  1, 'ðŸ¤¢': -1, 'ðŸ¤£':  0, 'ðŸ¤¤':  1, 'ðŸ¤¥': -1, 'ðŸ¤¦': -1,\n",
    "                   'ðŸ¤¦ðŸ»': -1, 'ðŸ¤¦ðŸ¼': -1, 'ðŸ¤¦ðŸ½': -1, 'ðŸ¤¦ðŸ¾': -1, 'ðŸ¤¦ðŸ¿': -1, 'ðŸ¤§':  0, 'ðŸ¤¨':  0,\n",
    "                   'ðŸ¤ª':  1, 'ðŸ¤«':  0, 'ðŸ¤¬': -1, 'ðŸ¤­':  1, 'ðŸ¤®': -1, 'ðŸ¤¯':  0, 'ðŸ¥°':  1,\n",
    "                   'ðŸ¥±':  0, 'ðŸ¥³':  1, 'ðŸ¥´':  1, 'ðŸ¥µ': -1, 'ðŸ¥¶': -1, 'ðŸ¥º': -1, 'ðŸ§':  0,\n",
    "                   'ðŸ§¡':  1}\n",
    "def clean_tweets(X):\n",
    "    cleaned_tweets = []\n",
    "    i = 0\n",
    "    for i, origtweet in enumerate(X):\n",
    "        i += 1\n",
    "        if i % (len(X)//10) == 0:\n",
    "            print(str(10*i/(len(X)//10))+'% read')\n",
    "\n",
    "        emojis = []\n",
    "        tweet = ''\n",
    "        no_emojis = True\n",
    "        for e in emoji_sentiment:\n",
    "            if e in origtweet:\n",
    "                no_emojis = False\n",
    "                for char in origtweet:\n",
    "                    if char == e:\n",
    "                        tweet += ' '+char+' ' # this is so we can parse out the emoji\n",
    "\n",
    "                        emojis.append(e)\n",
    "                    else:\n",
    "                        tweet += char\n",
    "        if no_emojis:\n",
    "            continue\n",
    "\n",
    "        tweet_cleaned = []\n",
    "        for word in clean_up_text(tweet).split(' '):\n",
    "            if word[0] != '@':\n",
    "                tweet_cleaned.append(word)\n",
    "            else:\n",
    "                tweet_cleaned.append('')\n",
    "        tweet_cleaned = ' '.join(tweet_cleaned)\n",
    "\n",
    "        cleaned_tweets.append(tweet_cleaned)\n",
    "        \n",
    "    return list(set(cleaned_tweets))\n",
    "\n",
    "cleaned_tweets = clean_tweets(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def split_tr_val_te(X):\n",
    "    percent_tr = 0.5\n",
    "    percent_val = 0.25\n",
    "\n",
    "    tr_size = batch_size*int(percent_tr*len(X)/batch_size)\n",
    "    val_size = int((len(X)-tr_size)*(percent_val/(1-percent_tr)))\n",
    "    te_size = len(X) - tr_size - val_size\n",
    "\n",
    "    X_val = X[:val_size]\n",
    "    X_tr = X[val_size:-te_size]\n",
    "    X_te = X[-te_size:]\n",
    "#     assert((len(X_tr)==len(y_tr)) and\n",
    "#            (len(X_val)==len(y_val)) and\n",
    "#            (len(X_te)==len(y_te)))\n",
    "    print(len(X_tr)/len(X),len(X_val)/len(X),len(X_te)/len(X),len(X_tr)/batch_size)\n",
    "    return X_tr, X_val, X_te\n",
    "\n",
    "cleaned_tweets_tr,cleaned_tweets_val,cleaned_tweets_te = split_tr_val_te(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get__X_and__y(cleaned_tweets):\n",
    "    _X = []\n",
    "    _y = []\n",
    "    not_emojis = 0\n",
    "    for i, tweet in enumerate(cleaned_tweets):\n",
    "        sentiments = []\n",
    "        not_emoji = False\n",
    "        for e in emoji_sentiment:\n",
    "            if 'not '+e in tweet:\n",
    "                not_emoji = True\n",
    "        if not_emoji:\n",
    "            not_emojis += 1\n",
    "            continue\n",
    "        emojis = []\n",
    "        for word in tweet.split(' '):\n",
    "            for e in emoji_sentiment:\n",
    "                if word == e:\n",
    "                    sentiments.append(emoji_sentiment[e])\n",
    "                    emojis.append(e)\n",
    "        # only include tweets that have exactly one of the selected emojis\n",
    "        if len(set(sentiments)) == 1:\n",
    "            for e in emojis:\n",
    "                tweet = tweet.replace(e,'')\n",
    "            _X.append(clean_up_text(tweet))\n",
    "            _y.append(sentiments[0])\n",
    "    return _X, _y\n",
    "_X_tr_strs, _y_tr = get__X_and__y(cleaned_tweets_tr)\n",
    "_X_val_strs, _y_val = get__X_and__y(cleaned_tweets_val)\n",
    "_X_te_strs, _y_te = get__X_and__y(cleaned_tweets_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweets_into_words(_X):\n",
    "    return [[word for word in tweet.split(' ')] for tweet in _X]\n",
    "_X_tr = split_tweets_into_words(_X_tr_strs)\n",
    "_X_val = split_tweets_into_words(_X_val_strs)\n",
    "_X_te = split_tweets_into_words(_X_te_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def get_most_frequent_keys(d,n):\n",
    "#     a = np.array(list(d.values()))\n",
    "#     thresh = min(a[np.argpartition(a,-n)][-n:])\n",
    "#     inds = a > 0.9 * thresh\n",
    "#     d = dict(np.array(list(d.items()))[inds])\n",
    "#     for k in d:\n",
    "#         d[k] = int(d[k])\n",
    "#     odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "#     return odct\n",
    "\n",
    "emoji_counts = {-1:0,0:0,1:0}\n",
    "\n",
    "for i in _y_tr:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in _X_tr]+\n",
    "                         [' '.join(tweet) for tweet in _X_val]+\n",
    "                         [' '.join(tweet) for tweet in _X_te]).split(' ')\n",
    "words = set(runonsentence)\n",
    "\n",
    "def get_counts_adv(_X):\n",
    "    d = dict(zip(words,[0]*len(words)))\n",
    "    for x in _X:\n",
    "        for w in x:\n",
    "            d[w] += 1\n",
    "    return d\n",
    "\n",
    "wc_tr = get_counts_adv(_X_tr)\n",
    "wc_val = get_counts_adv(_X_val)\n",
    "wc_te = get_counts_adv(_X_te)\n",
    "\n",
    "len(runonsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = []\n",
    "nonzeros = []\n",
    "for w in words:\n",
    "    if wc_tr[w]*wc_val[w]*wc_te[w]==0:\n",
    "        zeros.append(w)\n",
    "    else:\n",
    "        nonzeros.append(w)\n",
    "len(zeros),len(nonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runonsentence = np.array(runonsentence,dtype='str')\n",
    "\n",
    "for i,w in enumerate(zeros):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    runonsentence[runonsentence == w] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = get_counts(runonsentence)\n",
    "    \n",
    "assert(min(list(word_counts.values())) > 1)\n",
    "\n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([(0,len(x)) for x in _X_tr]+\n",
    "           [(1,len(x)) for x in _X_val]+\n",
    "           [(2,len(x)) for x in _X_te])\n",
    "T=np.vstack([T.T,np.arange(len(T))]).T\n",
    "tr=len(_X_tr)\n",
    "val=len(_X_val)\n",
    "T[:,2][tr:] -= tr\n",
    "T[:,2][tr+val:] -= val\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2],T[0:2],T[tr:tr+2],T[tr+val:tr+val+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for t in T:\n",
    "    for j in range(t[1]):\n",
    "        if t[0] == 0:\n",
    "            _X_tr[t[2]][j] = runonsentence[i]\n",
    "        elif t[0] == 1:\n",
    "            _X_val[t[2]][j] = runonsentence[i]\n",
    "        else:\n",
    "            _X_te[t[2]][j] = runonsentence[i]\n",
    "        i += 1\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def convert_to_id(_X):\n",
    "    return [[word2id[word] for word in tweet] for tweet in _X]\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    return to_categorical(_y, num_classes=3)\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "max_words = 140\n",
    "X_tr= sequence.pad_sequences(convert_to_id(_X_tr), maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(convert_to_id(_X_val), maxlen=max_words)\n",
    "X_te = sequence.pad_sequences(convert_to_id(_X_te), maxlen=max_words)\n",
    "\n",
    "y_tr = convert_to_one_hot(_y_tr)\n",
    "y_val = convert_to_one_hot(_y_val)\n",
    "y_te = convert_to_one_hot(_y_te)\n",
    "\n",
    "X_tr[:10], y_tr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.arange(3)-1, y=_y_tr)\n",
    "\n",
    "_y_tr[:10], class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "def calc_n_neurons():\n",
    "    alpha = np.log2(len(X_tr))\n",
    "    Ni = model.layers[-1].input.shape[-1]\n",
    "    No = model.layers[-1].output.shape[-1]\n",
    "    n_neurons = max(int(len(X_tr) / (2 * alpha * (Ni + No))), 1)\n",
    "    print(alpha,Ni,No,n_neurons)\n",
    "    return n_neurons\n",
    "\n",
    "model.add(LSTM(calc_n_neurons(), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(calc_n_neurons()))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 3\n",
    "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "yhat_te = np.array([model.predict(np.array(x)[np.newaxis]) for x in X_te])[:,0]\n",
    "y_te - yhat_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
