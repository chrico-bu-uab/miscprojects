{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_full_text(tweet):\n",
    "    if 'extended_tweet' in tweet:\n",
    "        return tweet['extended_tweet']['full_text']\n",
    "    else:\n",
    "        return tweet['text']\n",
    "    \n",
    "def clean_up_text(text):\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',' ')\n",
    "    return text.strip()\n",
    "        \n",
    "def get_orig_text(tweet):\n",
    "    if 'retweeted_status' in tweet:\n",
    "        return get_orig_text(tweet['retweeted_status'])\n",
    "    else:\n",
    "        return clean_up_text(get_full_text(tweet))\n",
    "    \n",
    "def get_raw_tweets(foldername,stop):\n",
    "    directory=[f for f in glob.iglob('Downloads/'+foldername+'/*')]\n",
    "    raw_tweets = []\n",
    "    i = 0\n",
    "    msg = True\n",
    "    for filepath in directory:\n",
    "        file = open(filepath, 'r')\n",
    "        for line in file:\n",
    "            \n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            text = get_orig_text(tweet)\n",
    "            \n",
    "            if ('http://' not in text) and ('https://' not in text):\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "                \n",
    "                raw_tweets.append(text)\n",
    "                msg = True\n",
    "                \n",
    "                if len(raw_tweets) >= stop:\n",
    "                    file.close()\n",
    "                    return list(set(raw_tweets))\n",
    "                \n",
    "                if (stop<float('inf')) and msg:\n",
    "                    if (len(raw_tweets) > 0) and (len(raw_tweets) % (stop//20) == 0):\n",
    "                        print(str(len(raw_tweets))+' of '+str(stop)+' tweets read')\n",
    "                    msg = False\n",
    "                \n",
    "        file.close()\n",
    "        \n",
    "        if stop==float('inf'):\n",
    "            if (i > 0) and (i % (len(directory)//20) == 0):\n",
    "                print(str(i)+' of '+str(len(directory))+' files read ('+\n",
    "                      str(len(raw_tweets))+' total tweets)')\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return list(set(raw_tweets))\n",
    "\n",
    "raw_tweets = get_raw_tweets('BackLAOut',float('inf'))\n",
    "raw_tweets += get_raw_tweets('BackNYOut',float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweets = ' '.join(raw_tweets)\n",
    "\n",
    "len(raw_tweets), raw_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def get_counts(i):\n",
    "    d = Counter()\n",
    "    for j in i:\n",
    "        d[j] += 1\n",
    "    return d\n",
    "\n",
    "raw_char_counts = get_counts(joined_tweets)\n",
    "raw_word_counts = get_counts(joined_tweets.split(' '))\n",
    "    \n",
    "def display_top_10_counts(d):\n",
    "    odct = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    for i, k in enumerate(list(odct)):\n",
    "        if i > 9:\n",
    "            del odct[k]\n",
    "    return odct\n",
    "\n",
    "display_top_10_counts(raw_char_counts), display_top_10_counts(raw_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sentiment = {'☹': -1, '☺':  1, '♥':  1, '❣':  1, '❤':  1, '🐱':  1, '🐵':  0, \n",
    "                   '👅':  1, '👺': -1, '👿': -1, '💑':  1, '💓':  1, '💔': -1, '💕':  1,\n",
    "                   '💖':  1, '💗':  1, '💘':  1, '💙': -1, '💚':  1, '💛':  1, '💜':  1,\n",
    "                   '💝':  1, '💞':  1, '💟':  1, '🖤': -1, '😀':  1, '😁':  1, '😂':  1,\n",
    "                   '😃':  1, '😄':  1, '😅':  1, '😆':  1, '😇':  1, '😈':  1, '😉':  1,\n",
    "                   '😊':  1, '😋':  1, '😌':  1, '😍':  1, '😎':  1, '😏':  1, '😐':  0,\n",
    "                   '😑': -1, '😒': -1, '😓': -1, '😔': -1, '😕': -1, '😖': -1, '😗':  1,\n",
    "                   '😘':  1, '😙':  1, '😚':  1, '😛':  1, '😜':  1, '😝':  1, '😞': -1,\n",
    "                   '😟': -1, '😠': -1, '😡': -1, '😢': -1, '😣': -1, '😤': -1, '😥': -1,\n",
    "                   '😦': -1, '😧': -1, '😨': -1, '😩': -1, '😪':  0, '😫': -1, '😬':  0,\n",
    "                   '😭': -1, '😮':  0, '😯':  0, '😰': -1, '😱': -1, '😲': -1, '😳': -1,\n",
    "                   '😴':  0, '😵': -1, '😶':  0, '😷':  0, '😸':  1, '😹':  0, '😺':  1,\n",
    "                   '😻':  1, '😼':  1, '😽':  1, '😾': -1, '😿': -1, '🙀':  0, '🙁': -1,\n",
    "                   '🙂':  1, '🙃':  1, '🙄': -1, '🙈':  0, '🙉':  0, '🙊':  0, '🤍':  1,\n",
    "                   '🤎':  1, '🤐':  0, '🤑':  1, '🤒': -1, '🤓':  1, '🤔':  0, '🤕': -1,\n",
    "                   '🤖':  0, '🤗':  1, '🤛':  1, '🤛🏻':  1, '🤛🏼':  1, '🤛🏽':  1, '🤛🏾':  1,\n",
    "                   '🤛🏿':  1, '🤜':  1, '🤜🏻':  1, '🤜🏼':  1, '🤜🏽':  1, '🤜🏾':  1, '🤜🏿':  1,\n",
    "                   '🤠':  1, '🤡':  1, '🤢': -1, '🤣':  0, '🤤':  1, '🤥': -1, '🤦': -1,\n",
    "                   '🤦🏻': -1, '🤦🏼': -1, '🤦🏽': -1, '🤦🏾': -1, '🤦🏿': -1, '🤧':  0, '🤨':  0,\n",
    "                   '🤪':  1, '🤫':  0, '🤬': -1, '🤭':  1, '🤮': -1, '🤯':  0, '🥰':  1,\n",
    "                   '🥱':  0, '🥳':  1, '🥴':  1, '🥵': -1, '🥶': -1, '🥺': -1, '🧐':  0,\n",
    "                   '🧡':  1}\n",
    "def clean_tweets(X):\n",
    "    cleaned_tweets = []\n",
    "    i = 0\n",
    "    for i, origtweet in enumerate(X):\n",
    "        i += 1\n",
    "        if i % (len(X)//10) == 0:\n",
    "            print(str(10*i/(len(X)//10))+'% read')\n",
    "\n",
    "        emojis = []\n",
    "        tweet = ''\n",
    "        no_emojis = True\n",
    "        for e in emoji_sentiment:\n",
    "            if e in origtweet:\n",
    "                no_emojis = False\n",
    "                for char in origtweet:\n",
    "                    if char == e:\n",
    "                        tweet += ' '+char+' ' # this is so we can parse out the emoji\n",
    "\n",
    "                        emojis.append(e)\n",
    "                    else:\n",
    "                        tweet += char\n",
    "        if no_emojis:\n",
    "            continue\n",
    "\n",
    "        tweet_cleaned = []\n",
    "        for word in clean_up_text(tweet).split(' '):\n",
    "            if word[0] != '@':\n",
    "                tweet_cleaned.append(word)\n",
    "            else:\n",
    "                tweet_cleaned.append('')\n",
    "        tweet_cleaned = ' '.join(tweet_cleaned)\n",
    "\n",
    "        cleaned_tweets.append(tweet_cleaned)\n",
    "        \n",
    "    return list(set(cleaned_tweets))\n",
    "\n",
    "cleaned_tweets = clean_tweets(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def split_tr_val_te(X):\n",
    "    percent_tr = 0.5\n",
    "    percent_val = 0.25\n",
    "\n",
    "    tr_size = batch_size*int(percent_tr*len(X)/batch_size)\n",
    "    val_size = int((len(X)-tr_size)*(percent_val/(1-percent_tr)))\n",
    "    te_size = len(X) - tr_size - val_size\n",
    "\n",
    "    X_val = X[:val_size]\n",
    "    X_tr = X[val_size:-te_size]\n",
    "    X_te = X[-te_size:]\n",
    "#     assert((len(X_tr)==len(y_tr)) and\n",
    "#            (len(X_val)==len(y_val)) and\n",
    "#            (len(X_te)==len(y_te)))\n",
    "    print(len(X_tr)/len(X),len(X_val)/len(X),len(X_te)/len(X),len(X_tr)/batch_size)\n",
    "    return X_tr, X_val, X_te\n",
    "\n",
    "cleaned_tweets_tr,cleaned_tweets_val,cleaned_tweets_te = split_tr_val_te(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get__X_and__y(cleaned_tweets):\n",
    "    _X = []\n",
    "    _y = []\n",
    "    not_emojis = 0\n",
    "    for i, tweet in enumerate(cleaned_tweets):\n",
    "        sentiments = []\n",
    "        not_emoji = False\n",
    "        for e in emoji_sentiment:\n",
    "            if 'not '+e in tweet:\n",
    "                not_emoji = True\n",
    "        if not_emoji:\n",
    "            not_emojis += 1\n",
    "            continue\n",
    "        emojis = []\n",
    "        for word in tweet.split(' '):\n",
    "            for e in emoji_sentiment:\n",
    "                if word == e:\n",
    "                    sentiments.append(emoji_sentiment[e])\n",
    "                    emojis.append(e)\n",
    "        # only include tweets that have exactly one of the selected emojis\n",
    "        if len(set(sentiments)) == 1:\n",
    "            for e in emojis:\n",
    "                tweet = tweet.replace(e,'')\n",
    "            _X.append(clean_up_text(tweet))\n",
    "            _y.append(sentiments[0])\n",
    "    return _X, _y\n",
    "_X_tr_strs, _y_tr = get__X_and__y(cleaned_tweets_tr)\n",
    "_X_val_strs, _y_val = get__X_and__y(cleaned_tweets_val)\n",
    "_X_te_strs, _y_te = get__X_and__y(cleaned_tweets_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweets_into_words(_X):\n",
    "    return [[word for word in tweet.split(' ')] for tweet in _X]\n",
    "_X_tr = split_tweets_into_words(_X_tr_strs)\n",
    "_X_val = split_tweets_into_words(_X_val_strs)\n",
    "_X_te = split_tweets_into_words(_X_te_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def get_most_frequent_keys(d,n):\n",
    "#     a = np.array(list(d.values()))\n",
    "#     thresh = min(a[np.argpartition(a,-n)][-n:])\n",
    "#     inds = a > 0.9 * thresh\n",
    "#     d = dict(np.array(list(d.items()))[inds])\n",
    "#     for k in d:\n",
    "#         d[k] = int(d[k])\n",
    "#     odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "#     return odct\n",
    "\n",
    "emoji_counts = {-1:0,0:0,1:0}\n",
    "\n",
    "for i in _y_tr:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts and vocabulary size\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in _X_tr]+\n",
    "                         [' '.join(tweet) for tweet in _X_val]+\n",
    "                         [' '.join(tweet) for tweet in _X_te]).split(' ')\n",
    "words = set(runonsentence)\n",
    "\n",
    "def get_counts_adv(_X):\n",
    "    d = dict(zip(words,[0]*len(words)))\n",
    "    for x in _X:\n",
    "        for w in x:\n",
    "            d[w] += 1\n",
    "    return d\n",
    "\n",
    "wc_tr = get_counts_adv(_X_tr)\n",
    "wc_val = get_counts_adv(_X_val)\n",
    "wc_te = get_counts_adv(_X_te)\n",
    "\n",
    "len(runonsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = []\n",
    "nonzeros = []\n",
    "for w in words:\n",
    "    if wc_tr[w]*wc_val[w]*wc_te[w]==0:\n",
    "        zeros.append(w)\n",
    "    else:\n",
    "        nonzeros.append(w)\n",
    "len(zeros),len(nonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runonsentence = np.array(runonsentence,dtype='str')\n",
    "\n",
    "for i,w in enumerate(zeros):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    runonsentence[runonsentence == w] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = get_counts(runonsentence)\n",
    "    \n",
    "assert(min(list(word_counts.values())) > 1)\n",
    "\n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([(0,len(x)) for x in _X_tr]+\n",
    "           [(1,len(x)) for x in _X_val]+\n",
    "           [(2,len(x)) for x in _X_te])\n",
    "T=np.vstack([T.T,np.arange(len(T))]).T\n",
    "tr=len(_X_tr)\n",
    "val=len(_X_val)\n",
    "T[:,2][tr:] -= tr\n",
    "T[:,2][tr+val:] -= val\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2],T[0:2],T[tr:tr+2],T[tr+val:tr+val+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for t in T:\n",
    "    for j in range(t[1]):\n",
    "        if t[0] == 0:\n",
    "            _X_tr[t[2]][j] = runonsentence[i]\n",
    "        elif t[0] == 1:\n",
    "            _X_val[t[2]][j] = runonsentence[i]\n",
    "        else:\n",
    "            _X_te[t[2]][j] = runonsentence[i]\n",
    "        i += 1\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def convert_to_id(_X):\n",
    "    return [[word2id[word] for word in tweet] for tweet in _X]\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    return to_categorical(_y, num_classes=3)\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "max_words = 140\n",
    "X_tr= sequence.pad_sequences(convert_to_id(_X_tr), maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(convert_to_id(_X_val), maxlen=max_words)\n",
    "X_te = sequence.pad_sequences(convert_to_id(_X_te), maxlen=max_words)\n",
    "\n",
    "y_tr = convert_to_one_hot(_y_tr)\n",
    "y_val = convert_to_one_hot(_y_val)\n",
    "y_te = convert_to_one_hot(_y_te)\n",
    "\n",
    "X_tr[:10], y_tr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.arange(3)-1, y=_y_tr)\n",
    "\n",
    "_y_tr[:10], class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "def calc_n_neurons():\n",
    "    alpha = np.log2(len(X_tr))\n",
    "    Ni = model.layers[-1].input.shape[-1]\n",
    "    No = model.layers[-1].output.shape[-1]\n",
    "    n_neurons = max(int(len(X_tr) / (2 * alpha * (Ni + No))), 1)\n",
    "    print(alpha,Ni,No,n_neurons)\n",
    "    return n_neurons\n",
    "\n",
    "model.add(LSTM(calc_n_neurons(), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(calc_n_neurons()))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 3\n",
    "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "yhat_te = np.array([model.predict(np.array(x)[np.newaxis]) for x in X_te])[:,0]\n",
    "y_te - yhat_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
