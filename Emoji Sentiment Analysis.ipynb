{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that performs a dynamic sentiment analysis based on emojis.\n",
    "\n",
    "Emojis are extracted from tweets, and the tweet text is used to predict the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 of 1000000.0 tweets read\n",
      "200000 of 1000000.0 tweets read\n",
      "300000 of 1000000.0 tweets read\n",
      "400000 of 1000000.0 tweets read\n",
      "500000 of 1000000.0 tweets read\n",
      "600000 of 1000000.0 tweets read\n",
      "700000 of 1000000.0 tweets read\n",
      "800000 of 1000000.0 tweets read\n",
      "900000 of 1000000.0 tweets read\n",
      "100000 of 1000000.0 tweets read\n",
      "200000 of 1000000.0 tweets read\n",
      "300000 of 1000000.0 tweets read\n",
      "400000 of 1000000.0 tweets read\n",
      "500000 of 1000000.0 tweets read\n",
      "600000 of 1000000.0 tweets read\n",
      "700000 of 1000000.0 tweets read\n",
      "800000 of 1000000.0 tweets read\n",
      "900000 of 1000000.0 tweets read\n"
     ]
    }
   ],
   "source": [
    "from extract_tweets import get_raw_tweets\n",
    "import pickle\n",
    "\n",
    "raw_tweets = get_raw_tweets('BackLAOut',1e6)\n",
    "raw_tweets += get_raw_tweets('BackNYOut',1e6)\n",
    "\n",
    "with open('raw_BackLA_BackNY', 'wb') as fp:\n",
    "    pickle.dump(raw_tweets, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0% read\n",
      "20.0% read\n",
      "30.0% read\n",
      "40.0% read\n",
      "50.0% read\n",
      "60.0% read\n",
      "70.0% read\n",
      "80.0% read\n",
      "90.0% read\n",
      "100.0% read\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228162"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from extract_tweets import clean_tweets\n",
    "import pickle\n",
    "with open('raw_BackLA_BackNY', 'rb') as fp:\n",
    "    raw_tweets = [tweet for tweet in pickle.load(fp)]\n",
    "    \n",
    "cleaned_tweets = clean_tweets(raw_tweets)\n",
    "\n",
    "with open('clean_BackLA_BackNY', 'wb') as fp:\n",
    "    pickle.dump(cleaned_tweets, fp)\n",
    "    \n",
    "len(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import emoji\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from extract_tweets import get__X_and__y\n",
    "import pickle\n",
    "with open('clean_BackLA_BackNY', 'rb') as fp:\n",
    "    cleaned_tweets = [tweet for tweet in pickle.load(fp)]\n",
    "\n",
    "random.shuffle(cleaned_tweets)\n",
    "\n",
    "emojis = emoji.core.unicode_codes.UNICODE_EMOJI\n",
    "emoji_sentiment1 = dict(zip(emojis, [0]*len(emojis)))\n",
    "emoji_sentiment2 = \\\n",
    "OrderedDict([('☹', -1),('👺', -1),('👿', -1),('💔', -1),('💙', -1),('🖤', -1),('😑', -1),\n",
    "             ('😒', -1),('😓', -1),('😔', -1),('😕', -1),('😖', -1),('😞', -1),('😟', -1),\n",
    "             ('😠', -1),('😡', -1),('😢', -1),('😣', -1),('😤', -1),('😥', -1),('😦', -1),\n",
    "             ('😧', -1),('😨', -1),('😩', -1),('😫', -1),('😭', -1),('😰', -1),('😱', -1),\n",
    "             ('😲', -1),('😳', -1),('😵', -1),('😾', -1),('😿', -1),('🙁', -1),('🙄', -1),\n",
    "             ('🤒', -1),('🤕', -1),('🤢', -1),('🤥', -1),('🤬', -1),('🤮', -1),('🥵', -1),\n",
    "             ('🥺', -1),('🤦', -1),('🤦🏻', -1),('🤦🏼', -1),('🤦🏽', -1),('🤦🏾', -1),('🤦🏿', -1),\n",
    "             \n",
    "             ('☺',  1),('♥',  1),('❣',  1),('❤',  1),('🐱',  1),('👅',  1),('💑',  1),\n",
    "             ('💓',  1),('💕',  1),('💖',  1),('💗',  1),('💘',  1),('💚',  1),('💛',  1),\n",
    "             ('💜',  1),('💝',  1),('💞',  1),('💟',  1),('😀',  1),('😁',  1),('😂',  1),\n",
    "             ('😃',  1),('😄',  1),('😅',  1),('😆',  1),('😇',  1),('😈',  1),('😉',  1),\n",
    "             ('😊',  1),('😋',  1),('😌',  1),('😍',  1),('😎',  1),('😏',  1),('😗',  1),\n",
    "             ('😘',  1),('😙',  1),('😚',  1),('😛',  1),('😜',  1),('😝',  1),('😸',  1),\n",
    "             ('😺',  1),('😻',  1),('😼',  1),('😽',  1),('🙂',  1),('🙃',  1),('🤍',  1),\n",
    "             ('🤎',  1),('🤑',  1),('🤗',  1),('🤛',  1),('🤛🏻',  1),('🤛🏼',  1),('🤛🏽',  1),\n",
    "             ('🤛🏾',  1),('🤛🏿',  1),('🤜',  1),('🤜🏻',  1),('🤜🏼',  1),('🤜🏽',  1),('🤜🏾',  1),\n",
    "             ('🤜🏿',  1),('🤠',  1),('🤡',  1),('🤤',  1),('🤪',  1),('🤭',  1),('🥰',  1),\n",
    "             ('🥳',  1),('🥴',  1),('🧡',  1)])\n",
    "for e in emojis:\n",
    "    if e in emoji_sentiment2:\n",
    "        del emoji_sentiment1[e]\n",
    "emoji_sentiment = OrderedDict(chain(emoji_sentiment1.items(),emoji_sentiment2.items()))\n",
    "\n",
    "_X, _y = get__X_and__y(cleaned_tweets, emoji_sentiment)\n",
    "\n",
    "with open('_X', 'wb') as fp:\n",
    "    pickle.dump(_X, fp)\n",
    "with open('_y', 'wb') as fp:\n",
    "    pickle.dump(_y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217341 0.3333379343980197 0.3333333333333333 0.33332873226864695 1132.0\n",
      "217341 0.3333379343980197 0.3333333333333333 0.33332873226864695 1132.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Counter({0: 22712, 1: 28380, -1: 21356}),\n",
       " {0: 0.31349381625441697, 1: 0.3917292402826855, -1: 0.2947769434628975})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import ceil\n",
    "import pickle\n",
    "with open('_X', 'rb') as fp:\n",
    "    _X = [tweet for tweet in pickle.load(fp)]\n",
    "with open('_y', 'rb') as fp:\n",
    "    _y = pickle.load(fp)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def split_tr_val_te(X):\n",
    "    percent_tr = 1/3\n",
    "    percent_val = 1/3\n",
    "\n",
    "    tr_size = batch_size*ceil(percent_tr*len(X)/batch_size)\n",
    "    val_size = ceil(percent_val*(len(X)-tr_size)/(1-percent_tr))\n",
    "    te_size = len(X) - tr_size - val_size\n",
    "\n",
    "    X_val = X[:val_size]\n",
    "    X_tr = X[val_size:-te_size]\n",
    "    X_te = X[-te_size:]\n",
    "    print(len(X),len(X_tr)/len(X),len(X_val)/len(X),len(X_te)/len(X),len(X_tr)/batch_size)\n",
    "    return X_tr, X_val, X_te\n",
    "\n",
    "_X_tr,_X_val,_X_te = split_tr_val_te(_X)\n",
    "_y_tr,_y_val,_y_te = split_tr_val_te(_y)\n",
    "\n",
    "with open('_X_tr', 'wb') as fp:\n",
    "    pickle.dump(_X_tr, fp)\n",
    "with open('_y_tr', 'wb') as fp:\n",
    "    pickle.dump(_y_tr, fp)\n",
    "with open('_X_val', 'wb') as fp:\n",
    "    pickle.dump(_X_val, fp)\n",
    "with open('_y_val', 'wb') as fp:\n",
    "    pickle.dump(_y_val, fp)\n",
    "with open('_X_te', 'wb') as fp:\n",
    "    pickle.dump(_X_te, fp)\n",
    "with open('_y_te', 'wb') as fp:\n",
    "    pickle.dump(_y_te, fp)\n",
    "\n",
    "def split_tweets_into_words(_X):\n",
    "    return [[word for word in tweet.split(' ')] for tweet in _X]\n",
    "\n",
    "_X_tr = split_tweets_into_words(_X_tr)\n",
    "_X_val = split_tweets_into_words(_X_val)\n",
    "_X_te = split_tweets_into_words(_X_te)\n",
    "\n",
    "emoji_counts = Counter()\n",
    "for i in _y_tr:\n",
    "    emoji_counts[i] += 1\n",
    "emoji_counts, {k:v/sum(emoji_counts.values()) for k,v in emoji_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.548947578913066 111.89735890667292\n",
      "4.311349261601813 95.5891048376753\n",
      "4.517848943135193 109.82640443575451\n",
      "4.2930707588393 94.70264922095481\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# # get word counts and vocabulary size\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in _X_tr]+\n",
    "                         [' '.join(tweet) for tweet in _X_val]+\n",
    "                         [' '.join(tweet) for tweet in _X_te]).split(' ')\n",
    "\n",
    "words = np.unique(runonsentence)\n",
    "def get_counts_adv(_X):\n",
    "    d1 = dict(zip(words,[0]*len(words)))\n",
    "    d2 = dict(zip(words,[0]*len(words)))\n",
    "    d3 = dict(zip(words,[0]*len(words)))\n",
    "    \n",
    "    for x in _X:\n",
    "        for w in x:\n",
    "            d1[w] += 1\n",
    "    m1 = np.mean(list(d1.values()))\n",
    "    sd1 = np.std(list(d1.values()))\n",
    "    print(m1,sd1)\n",
    "    \n",
    "    for x in _X:\n",
    "        d = Counter()\n",
    "        for w in x:\n",
    "            d[w] = 1\n",
    "        for w in d:\n",
    "            d2[w] += d[w]\n",
    "    m2 = np.mean(list(d2.values()))\n",
    "    sd2 = np.std(list(d2.values()))\n",
    "    print(m2,sd2)\n",
    "    \n",
    "    fdfdsf = list(reduce(lambda i, j: set(i) | set(j), _X))\n",
    "    for w in fdfdsf:\n",
    "        d3[w] = (d1[w] - m1) / sd1 + (d2[w] - m2) / sd2\n",
    "\n",
    "    return d3\n",
    "\n",
    "wc_tr = get_counts_adv(_X_tr)\n",
    "wc_val = get_counts_adv(_X_val)\n",
    "wc_te = get_counts_adv(_X_te)\n",
    "\n",
    "with open('wc_tr', 'wb') as fp:\n",
    "    pickle.dump(wc_tr, fp)\n",
    "with open('wc_val', 'wb') as fp:\n",
    "    pickle.dump(wc_val, fp)\n",
    "with open('wc_te', 'wb') as fp:\n",
    "    pickle.dump(wc_te, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open('_X_tr', 'rb') as fp:\n",
    "    _X_tr = pickle.load(fp)\n",
    "with open('_X_val', 'rb') as fp:\n",
    "    _X_val = pickle.load(fp)\n",
    "with open('_X_te', 'rb') as fp:\n",
    "    _X_te = pickle.load(fp)\n",
    "with open('_y_tr', 'rb') as fp:\n",
    "    _y_tr = pickle.load(fp)\n",
    "with open('_y_val', 'rb') as fp:\n",
    "    _y_val = pickle.load(fp)\n",
    "with open('_y_te', 'rb') as fp:\n",
    "    _y_te = pickle.load(fp)\n",
    "with open('wc_tr', 'rb') as fp:\n",
    "    wc_tr = pickle.load(fp)\n",
    "with open('wc_val', 'rb') as fp:\n",
    "    wc_val = pickle.load(fp)\n",
    "with open('wc_te', 'rb') as fp:\n",
    "    wc_te = pickle.load(fp)\n",
    "\n",
    "def get_most_frequent_keys(d,n):\n",
    "    a = np.array(list(d.values()),dtype='float')\n",
    "    a -= np.nanmin(a)\n",
    "    N = min(n,len(d))\n",
    "    inds = a >= 0.99 * min(a[np.argpartition(a,-N)][-N:])\n",
    "    d = dict(np.array(list(d.items()))[inds])\n",
    "    for k in d:\n",
    "        d[k] = float(d[k])\n",
    "    odct = OrderedDict(sorted(d.items(),key=lambda x:x[1], reverse=True))\n",
    "    return odct\n",
    "\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in _X_tr]+\n",
    "                         [' '.join(tweet) for tweet in _X_val]+\n",
    "                         [' '.join(tweet) for tweet in _X_te]).split(' ')\n",
    "words = np.unique(runonsentence)\n",
    "for w in words:\n",
    "    if wc_tr[w]*wc_val[w]*wc_te[w]==0:\n",
    "        del wc_tr[w]\n",
    "        del wc_val[w]\n",
    "        del wc_te[w]\n",
    "        \n",
    "vocab = get_most_frequent_keys(wc_tr,len(words)//20)\n",
    "print(len(vocab))\n",
    "\n",
    "runonsentence = [w if w in vocab else '<UNK>' for w in runonsentence]\n",
    "\n",
    "def get_counts(i):\n",
    "    d = Counter()\n",
    "    for j in i:\n",
    "        d[j] += 1\n",
    "    return d\n",
    "\n",
    "word_counts = get_counts(runonsentence)\n",
    "    \n",
    "assert(min(list(word_counts.values())) > 1)\n",
    "\n",
    "word_counts = OrderedDict(sorted(word_counts.items()), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "vocabulary_size = len(word_counts)\n",
    "vocabulary_size, word_counts['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([(0,len(x)) for x in _X_tr]+\n",
    "           [(1,len(x)) for x in _X_val]+\n",
    "           [(2,len(x)) for x in _X_te])\n",
    "T=np.vstack([T.T,np.arange(len(T))]).T\n",
    "tr=len(_X_tr)\n",
    "val=len(_X_val)\n",
    "T[:,2][tr:] -= tr\n",
    "T[:,2][tr+val:] -= val\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2],T[0:2],T[tr:tr+2],T[tr+val:tr+val+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for t in T:\n",
    "    for j in range(t[1]):\n",
    "        if t[0] == 0:\n",
    "            _X_tr[t[2]][j] = runonsentence[i]\n",
    "        elif t[0] == 1:\n",
    "            _X_val[t[2]][j] = runonsentence[i]\n",
    "        else:\n",
    "            _X_te[t[2]][j] = runonsentence[i]\n",
    "        i += 1\n",
    "_X_tr[0:2],_X_val[0:2],_X_te[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def convert_to_id(_X):\n",
    "    return [[word2id[word] for word in tweet] for tweet in _X]\n",
    "\n",
    "def convert_to_one_hot(_y):\n",
    "    #return enc.transform(np.array(_y).reshape(-1,1)).toarray()\n",
    "    _y = np.array(_y) - np.array(_y).min()\n",
    "    _y = _y * (len(np.unique(_y))-1) / max(_y)\n",
    "    print(_y)\n",
    "    return to_categorical(_y, num_classes=len(np.unique(list(emoji_sentiment.values()))))\n",
    "\n",
    "word2id = {}\n",
    "for i,word in enumerate(word_counts):\n",
    "    word2id[word] = i\n",
    "\n",
    "max_words = max([len(x) for x in _X])\n",
    "X_tr= sequence.pad_sequences(convert_to_id(_X_tr), maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(convert_to_id(_X_val), maxlen=max_words)\n",
    "X_te = sequence.pad_sequences(convert_to_id(_X_te), maxlen=max_words)\n",
    "\n",
    "y_tr = convert_to_one_hot(_y_tr)\n",
    "y_val = convert_to_one_hot(_y_val)\n",
    "y_te = convert_to_one_hot(_y_te)\n",
    "\n",
    "X_tr[:20], y_tr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the data grooming. Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.arange(len(y_tr[0])),\n",
    "                                                  y=np.argmax(y_tr,axis=1))\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                   classes=np.arange(len(y_tr[0]))-1,\n",
    "#                                                   y=_y_tr)\n",
    "\n",
    "print(np.sum(y_tr,axis=0)*class_weights)\n",
    "\n",
    "class_weights = {i:w for i,w in zip(range(len(y_tr[0])),class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "embedding_size=32\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "n_neurons = ceil(len(X_tr) / (embedding_size * len(class_weights)))\n",
    "forward_layer = LSTM(n_neurons)\n",
    "backward_layer = LSTM(n_neurons, activation='relu', go_backwards=True)\n",
    "model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "model.add(Dropout(0.36))\n",
    "if len(class_weights) == 2:\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "else:\n",
    "    model.add(Dense(len(class_weights), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy' if len(class_weights) == 2 else 'categorical_crossentropy'\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "num_epochs = 25\n",
    "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), batch_size=batch_size,\n",
    "          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "yhat_te = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=np.argmax(yhat_te,axis=1)==np.argmax(y_te,axis=1)\n",
    "yhat_te,y_te,score,np.mean(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
