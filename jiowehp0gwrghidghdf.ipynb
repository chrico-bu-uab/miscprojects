#!/usr/bin/env python
# coding: utf-8

# In[ ]:


from tensorflow.compat import v1 as tf
import numpy as np
import random
import os

import re
from extract_tweets import clean_up_text

import pickle

SEED = 42

tf.set_random_seed(SEED)
tf.random_normal_initializer(SEED)
tf.random_uniform_initializer(SEED)

np.random.seed(SEED)

random.seed(SEED)

os.environ['PYTHONHASHSEED'] =  str(SEED)
os.environ['CUDA_VISIBLE_DEVICES'] = str(SEED)
os.environ['TF_CUDNN_USE_AUTOTUNE'] = str(SEED)

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
tf.keras.backend.set_session(sess)

with open('Downloads/training.1600000.processed.noemoticon.csv', 'rb') as fopen:
    file = fopen.readlines()

inds = np.arange(len(file))
np.random.shuffle(inds)
file = np.array(file)[inds]

X_raw = np.empty(len(file), dtype='object')
y_raw = np.empty(len(file), dtype='int')
j = 0
fsdfklsd = 0
for i,line in enumerate(file):
    ls = str(line.decode("latin-1")).split('"')
    
    text = ls[-2]
    
    if ('http://' not in text) and ('https://' not in text):
        
        if fsdfklsd < 10:
            print(text)
            fsdfklsd += 1
        
        text = text.lower()
        text = re.sub('&', ' <and> ', text)
        text = re.sub('amp;', ' ', text)
        text = re.sub("'n'", ' <and> ', text)
        text = re.sub(' and$', ' <and> ', text)
        text = re.sub('^and ', ' <and> ', text)
        text = re.sub(' and ', ' <and> ', text)
        text = re.sub('â€¦', ' <ellipsis> ', text)
        text = re.sub('\.\.\.', ' <ellipsis> ', text)
        text = re.sub('@[a-z0-9]+', ' <handle> ', text)
        text = re.sub('#[a-z0-9]+', ' <hashtag> ', text)
        text = re.sub('@', ' <at> ', text)
        text = re.sub('#', ' <pound> ', text)
        text = re.sub('[\!]', ' <exclam> ', text)
        text = re.sub('[\?]', ' <quest> ', text)
        text = re.sub('[^\w\s\'\"\<\>]', ' <punct> ', text)
        text = clean_up_text(text)
        
        X_raw[j] = text
        y_raw[j] = int(ls[1]) / 4
        j += 1
    
X_raw = X_raw[:j]
y_raw = y_raw[:j]

with open('X_training.processed.noemoticon.csv', 'wb') as fp:
    pickle.dump(X_raw, fp)
with open('y_training.processed.noemoticon.csv', 'wb') as fp:
    pickle.dump(y_raw, fp)

X_raw[:10], y_raw[:10], np.unique(y_raw), len(y_raw)


# In[ ]:


from collections import Counter
import numpy as np
import pickle
with open('X_training.processed.noemoticon.csv', 'rb') as fp:
    X_raw = [tweet for tweet in pickle.load(fp)]
with open('y_training.processed.noemoticon.csv', 'rb') as fp:
    y_raw = pickle.load(fp)

batch_size = 64

def split_tr_val_te(X):
    percent_tr = 0.5
    percent_val = 0.25

    tr_size = batch_size*np.ceil(percent_tr*len(X)/batch_size)
    tr_size = np.array(tr_size, dtype='int')
    val_size = np.ceil(percent_val*(len(X)-tr_size)/(1-percent_tr))
    val_size = np.array(val_size, dtype='int')
    te_size = len(X) - tr_size - val_size

    X_val = X[:val_size]
    X_tr = X[val_size:-te_size]
    X_te = X[-te_size:]
    
    return X_tr, X_val, X_te

X_raw_tr, X_raw_val, X_raw_te = split_tr_val_te(X_raw)
y_raw_tr, y_raw_val, y_raw_te = split_tr_val_te(y_raw)


# In[ ]:


def split_tweets_into_words(X):
    return [[word for word in tweet.split(' ')] for tweet in X]

X_raw_tr = split_tweets_into_words(X_raw_tr)
X_raw_val = split_tweets_into_words(X_raw_val)
X_raw_te = split_tweets_into_words(X_raw_te)

(X_raw_tr[0], X_raw_val[0], X_raw_te[0],
 np.unique(y_raw_tr), np.unique(y_raw_val), np.unique(y_raw_te))


# In[ ]:


# get word counts and vocabulary size
runonsentence = ' '.join([' '.join(tweet) for tweet in X_raw_tr]+
                         [' '.join(tweet) for tweet in X_raw_val]+
                         [' '.join(tweet) for tweet in X_raw_te]).split(' ')

i = 0
j = 0
INCREMENT = 1000000
words = set()
while len(runonsentence[j:]) > 0:
    words = words.union(runonsentence[j:j+INCREMENT])
    i += 1
    j = INCREMENT * i
words = list(words)
words[:10], len(words)


# In[ ]:


def get_counts_adv(X):
    d1 = dict(zip(words,[0]*len(words)))
    d2 = dict(zip(words,[0]*len(words)))
    d3 = dict(zip(words,[0]*len(words)))
    
    for x in X:
        for w in x:
            d1[w] += 1
    m1 = np.mean(list(d1.values()))
    sd1 = np.std(list(d1.values()))
    print(m1,sd1)
    
    for x in X:
        d = Counter()
        for w in x:
            d[w] = 1
        for w in d:
            d2[w] += d[w]
    m2 = np.mean(list(d2.values()))
    sd2 = np.std(list(d2.values()))
    print(m2,sd2)
    
    for w in set(w for x in X for w in x):
        d3[w] = np.exp((d2[w] - m2) / sd2 - 0.1 * (d1[w] - m1) / sd1)

    return d3

wc_tr = get_counts_adv(X_raw_tr)
wc_val = get_counts_adv(X_raw_val)
wc_te = get_counts_adv(X_raw_te)

for w in words:
    if wc_tr[w]*wc_val[w]*wc_te[w]==0:
        del wc_tr[w]
        del wc_val[w]
        del wc_te[w]

def get_most_frequent_keys(d,n):
    a = np.array(list(d.values()),dtype='float')
    a -= np.nanmin(a)
    N = min(n,len(d))
    p = min(a[np.argpartition(a,-N)][-N:])
    inds = a >= 0.9 * p
    d = dict(np.array(list(d.items()))[inds])
    for k in d:
        d[k] = float(d[k])
    return d

vocab = get_most_frequent_keys(wc_tr,5000)
print(len(vocab))

def get_counts(i):
    d = Counter()
    for j in i:
        d[j] += 1
    return d

runonsentence = [w if w in vocab else '<UNK>' for w in runonsentence]
word_counts = get_counts(runonsentence)
    
assert(min(list(word_counts.values())) > 1)

vocabulary_size = len(word_counts)

vocabulary_size, word_counts['<UNK>']


# In[ ]:


T=np.array([(0,len(x)) for x in X_raw_tr]+
           [(1,len(x)) for x in X_raw_val]+
           [(2,len(x)) for x in X_raw_te])
T=np.vstack([T.T,np.arange(len(T))]).T
tr=len(X_raw_tr)
val=len(X_raw_val)
T[:,2][tr:] -= tr
T[:,2][tr+val:] -= val

i = 0
for t in T:
    for j in range(t[1]):
        if t[0] == 0:
            X_raw_tr[t[2]][j] = runonsentence[i]
        elif t[0] == 1:
            X_raw_val[t[2]][j] = runonsentence[i]
        else:
            X_raw_te[t[2]][j] = runonsentence[i]
        i += 1


# In[ ]:


from tensorflow.keras.preprocessing import sequence
# from tensorflow.keras.utils import to_categorical

word2id = {word:i for i, word in enumerate(word_counts)}

def convert_to_id(X_raw):
    return [[word2id[word] for word in tweet] for tweet in X_raw]

def convert_to_one_hot(y):
    #return enc.transform(np.array(y_raw).reshape(-1,1)).toarray()
    n_classes = len(set(y))
    assert(n_classes > 1)
    return y if n_classes == 2 else to_categorical(y, num_classes=n_classes)

max_words = max([len(x) for x in X_raw_tr])

X_tr= sequence.pad_sequences(convert_to_id(X_raw_tr), maxlen=max_words)
X_val = sequence.pad_sequences(convert_to_id(X_raw_val), maxlen=max_words)
X_te = sequence.pad_sequences(convert_to_id(X_raw_te), maxlen=max_words)

y_tr = convert_to_one_hot(y_raw_tr)
y_val = convert_to_one_hot(y_raw_val)
y_te = convert_to_one_hot(y_raw_te)

X_tr[:10], y_tr[:10], max_words


# In[ ]:


from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight('balanced',
                                                  classes=[0,1],
                                                  y=y_tr)
# class_weights = class_weight.compute_class_weight('balanced',
#                                                   classes=np.arange(len(y_tr[0]))-1,
#                                                   y=_y_tr)

print(np.sum([y_tr==0])*class_weights[0]-np.sum(y_tr[y_tr==1])*class_weights[1])

class_weights = {i:w for i,w in zip(range(len(y_tr)),class_weights)}

class_weights


# In[ ]:


from tensorflow.keras import Model
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, Bidirectional, BatchNormalization, Dense, Dropout
)

embedding_size = 32

TR_SIZE_FACTOR = 1
base_units = 0.01 * len(class_weights) *              (1 - np.exp(-TR_SIZE_FACTOR*len(X_tr))) *              np.sqrt(vocabulary_size*embedding_size)
base_units = np.ceil(base_units)

np.random.seed(SEED + 1)
units_a_backward, units_a_forward = np.random.randint(1, base_units, 2)
units_b, units_c = np.random.randint(1 , base_units, 2)
print(units_a_backward, units_a_forward, units_b, units_c)

dropout_dense = 0.2
dropout_lstm = 1 - np.sqrt(1 - dropout_dense)
dropout_bidir = 1 - np.sqrt(1 - dropout_lstm)

layer0         = Embedding(vocabulary_size, embedding_size, input_length=max_words)

backward_layer = LSTM(units_a_backward,
                      dropout=dropout_bidir, recurrent_dropout=dropout_bidir,
                      return_sequences=True,
                      go_backwards=True)                     
forward_layer  = LSTM(units_a_forward,
                      dropout=dropout_bidir, recurrent_dropout=dropout_bidir,
                      return_sequences=True)
layer1         = Bidirectional(forward_layer, backward_layer=backward_layer)
                      
layer2         = LSTM(units_b, dropout=dropout_lstm, recurrent_dropout=dropout_lstm)

layer3         = BatchNormalization()

layer4         = Dense(units_c, activation='relu')

layer5         = Dropout(dropout_dense)

layer6         = Dense(1 if len(class_weights) == 2 else len(class_weights),
                       activation='sigmoid'if len(class_weights) == 2 else 'softmax')

inputs = Input(batch_input_shape=(None, None))
x = layer0(inputs)
x = layer1(x)
x = layer2(x)
x = layer3(x)
x = layer4(x)
x = layer5(x)
outputs = layer6(x)

model=Model(inputs=inputs, outputs=outputs)

model.summary()


# In[ ]:


loss = 'binary_crossentropy' if len(class_weights) == 2 else 'categorical_crossentropy'
model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])


# In[ ]:


from tensorflow.keras.callbacks import ModelCheckpoint
filepath = 'temp.hdf5'
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,
                             save_best_only=True, mode='max')
num_epochs = 25
print(len(X_tr))
model.fit(X_tr, y_tr, validation_data=(X_val, y_val), batch_size=batch_size,
          epochs=num_epochs, class_weight=class_weights, callbacks=[checkpoint])


# In[ ]:


model.load_weights(filepath)

yhat_te = model.predict(X_te)

scores = model.evaluate(X_te, y_te, verbose=0)

yhat_te[:10], 'Test accuracy:', scores[1]
