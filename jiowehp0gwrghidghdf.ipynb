{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.compat import v1 as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "import re\n",
    "from extract_tweets import clean_up_text\n",
    "\n",
    "import pickle\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def getSeed():\n",
    "    return np.random.randint(1, 2**32) - 1\n",
    "\n",
    "tf.set_random_seed(getSeed())\n",
    "tf.random_normal_initializer(getSeed())\n",
    "tf.random_uniform_initializer(getSeed())\n",
    "\n",
    "random.seed(getSeed())\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] =  str(getSeed())\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(getSeed())\n",
    "os.environ['TF_CUDNN_USE_AUTOTUNE'] = str(getSeed())\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "with open('Downloads/training.1600000.processed.noemoticon.csv', 'rb') as fopen:\n",
    "    file = fopen.readlines()\n",
    "\n",
    "file1 = deepcopy(file)    \n",
    "file1 = [str(line.decode(\"latin-1\")).split('\"') for line in file1]\n",
    "\n",
    "df = pd.DataFrame(file1)[[1,5,9,11]]\n",
    "df.columns = ['target', 'date', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '4'], dtype=object),\n",
       " array(['Mon Apr 06 22:19:45 PDT 2009', 'Mon Apr 06 22:19:49 PDT 2009',\n",
       "        'Mon Apr 06 22:19:53 PDT 2009', ...,\n",
       "        'Tue Jun 16 08:38:58 PDT 2009', 'Tue Jun 16 08:39:00 PDT 2009',\n",
       "        'Tue Jun 16 08:40:50 PDT 2009'], dtype=object),\n",
       " array(['_TheSpecialOne_', 'scotthamilton', 'mattycus', ..., 'EvolveTom',\n",
       "        'AmandaMarie1028', 'bpbabe'], dtype=object),\n",
       " array([\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\",\n",
       "        \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n",
       "        '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds',\n",
       "        ..., 'Are you ready for your MoJo Makeover? Ask me for details ',\n",
       "        'Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur ',\n",
       "        'happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H '],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].unique(),df['date'].unique(),df['user'].unique(),df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['<handle> the weekend is nigh',\n",
       "        \"omg <exclam> i'm so keen on going to laser runner over the long weekend <exclam> it looks freaking awesome\",\n",
       "        'had a totally epic conversation with at the grotto just now',\n",
       "        'summer i love adam brody', '<handle> i cant go either'],\n",
       "       dtype=object),\n",
       " array([1, 1, 1, 1, 0]),\n",
       " array([0, 1]),\n",
       " 55556)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_SAMPLE_SPACE = np.ceil(1529881 / 0.9 * 0.03268228051)\n",
    "\n",
    "inds = np.arange(len(file))\n",
    "np.random.shuffle(inds)\n",
    "file = np.array(file)[inds]\n",
    "\n",
    "X_raw = np.empty(len(file), dtype='object')\n",
    "y_raw = np.empty(len(file), dtype='int')\n",
    "j = 0\n",
    "\n",
    "for i,line in enumerate(file):\n",
    "    \n",
    "    ls = str(line.decode(\"latin-1\")).split('\"')\n",
    "    \n",
    "    text = ls[-2]\n",
    "    \n",
    "    if ('http://' not in text) and ('https://' not in text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = re.sub('&', ' <and> ', text)\n",
    "        text = re.sub('amp;', ' ', text)\n",
    "        text = re.sub(\"'n'\", ' <and> ', text)\n",
    "        text = re.sub(' and$', ' <and> ', text)\n",
    "        text = re.sub('^and ', ' <and> ', text)\n",
    "        text = re.sub(' and ', ' <and> ', text)\n",
    "        text = re.sub('â€¦', ' <ellipsis> ', text)\n",
    "        text = re.sub('\\.\\.\\.', ' <ellipsis> ', text)\n",
    "        text = re.sub('@[a-z0-9]+', ' <handle> ', text)\n",
    "        text = re.sub('#[a-z0-9]+', ' <hashtag> ', text)\n",
    "        text = re.sub('@', ' <at> ', text)\n",
    "        text = re.sub('#', ' <pound> ', text)\n",
    "        text = re.sub('[\\!]', ' <exclam> ', text)\n",
    "        text = re.sub('[\\?]', ' <quest> ', text)\n",
    "        text = re.sub('[^\\w\\s\\'\\\"\\<\\>]', ' <punct> ', text)\n",
    "        text = clean_up_text(text)\n",
    "        \n",
    "        X_raw[j] = text\n",
    "        y_raw[j] = int(ls[1]) / 4\n",
    "        j += 1\n",
    "    if j == TOTAL_SAMPLE_SPACE: break\n",
    "    \n",
    "X_raw = X_raw[:j]\n",
    "y_raw = y_raw[:j]\n",
    "\n",
    "with open('X_training.processed.noemoticon.csv', 'wb') as fp:\n",
    "    pickle.dump(X_raw, fp)\n",
    "with open('y_training.processed.noemoticon.csv', 'wb') as fp:\n",
    "    pickle.dump(y_raw, fp)\n",
    "\n",
    "X_raw[:5], y_raw[:5], np.unique(y_raw), len(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open('X_training.processed.noemoticon.csv', 'rb') as fp:\n",
    "    X_raw = [tweet for tweet in pickle.load(fp)]\n",
    "with open('y_training.processed.noemoticon.csv', 'rb') as fp:\n",
    "    y_raw = pickle.load(fp)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def split_tr_val_te(X):\n",
    "    percent_tr = 0.9\n",
    "    te_size = batch_size\n",
    "\n",
    "    tr_size = batch_size*int(np.ceil(percent_tr*len(X)/batch_size))\n",
    "    val_size = len(X) - tr_size - te_size\n",
    "    assert(val_size > 0)\n",
    "\n",
    "    X_val = X[:val_size]\n",
    "    X_tr = X[val_size:-te_size]\n",
    "    X_te = X[-te_size:]\n",
    "    \n",
    "    return X_tr, X_val, X_te\n",
    "\n",
    "X_raw_tr, X_raw_val, X_raw_te = split_tr_val_te(X_raw)\n",
    "y_raw_tr, y_raw_val, y_raw_te = split_tr_val_te(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ugh',\n",
       "  '<exclam>',\n",
       "  'i',\n",
       "  'rolled',\n",
       "  'out',\n",
       "  'of',\n",
       "  'bed',\n",
       "  'this',\n",
       "  'morning',\n",
       "  '<and>',\n",
       "  'kelp',\n",
       "  'rolling',\n",
       "  'til',\n",
       "  'i',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'wall',\n",
       "  '<exclam>',\n",
       "  'my',\n",
       "  'head',\n",
       "  'still',\n",
       "  'hurt',\n",
       "  'then',\n",
       "  'i',\n",
       "  'got',\n",
       "  'water',\n",
       "  'to',\n",
       "  'drank',\n",
       "  '<and>',\n",
       "  'missed',\n",
       "  'my',\n",
       "  'face'],\n",
       " ['<handle>', 'the', 'weekend', 'is', 'nigh'],\n",
       " ['anyways',\n",
       "  'sorry',\n",
       "  'twitters',\n",
       "  '<punct>',\n",
       "  'goodmorning',\n",
       "  'the',\n",
       "  'weathers',\n",
       "  'nice',\n",
       "  'here',\n",
       "  'already',\n",
       "  '<punct>',\n",
       "  'i',\n",
       "  'wanna',\n",
       "  'go',\n",
       "  'be',\n",
       "  'a',\n",
       "  'beach',\n",
       "  'bum',\n",
       "  '<punct>',\n",
       "  'lol',\n",
       "  '<punct>'],\n",
       " array([0, 1]),\n",
       " array([0, 1]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_tweets_into_words(X):\n",
    "    return [[word for word in tweet.split(' ')] for tweet in X]\n",
    "\n",
    "X_raw_tr = split_tweets_into_words(X_raw_tr)\n",
    "X_raw_val = split_tweets_into_words(X_raw_val)\n",
    "X_raw_te = split_tweets_into_words(X_raw_te)\n",
    "\n",
    "(X_raw_tr[0], X_raw_val[0], X_raw_te[0],\n",
    " np.unique(y_raw_tr), np.unique(y_raw_val), np.unique(y_raw_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['beckkyy', 'helpe', 'replace', 'mv', 'finishing'], 40926)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word counts and vocabulary size\n",
    "runonsentence = ' '.join([' '.join(tweet) for tweet in X_raw_tr]+\n",
    "                         [' '.join(tweet) for tweet in X_raw_val]+\n",
    "                         [' '.join(tweet) for tweet in X_raw_te]).split(' ')\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "INCREMENT = 1000000\n",
    "words = set()\n",
    "while len(runonsentence[j:]) > 0:\n",
    "    words = words.union(runonsentence[j:j+INCREMENT])\n",
    "    i += 1\n",
    "    j = INCREMENT * i\n",
    "words = list(words)\n",
    "words[:5], len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.316571372721498 468.057692628304\n",
      "16.90030787274593 295.54582030197133\n",
      "5111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5112, 89561, 289, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_counts_adv(X):\n",
    "    d1 = dict(zip(words,[0]*len(words)))\n",
    "    d2 = dict(zip(words,[0]*len(words)))\n",
    "    d3 = dict(zip(words,[0]*len(words)))\n",
    "    \n",
    "    for x in X:\n",
    "        for w in x:\n",
    "            d1[w] += 1\n",
    "            \n",
    "    m1 = np.mean(list(d1.values()))\n",
    "    sd1 = np.std(list(d1.values()))\n",
    "    print(m1,sd1)\n",
    "    \n",
    "    for x in X:\n",
    "        d = Counter()\n",
    "        for w in x:\n",
    "            d[w] = 1\n",
    "        for w in d:\n",
    "            d2[w] += d[w]\n",
    "            \n",
    "    m2 = np.mean(list(d2.values()))\n",
    "    sd2 = np.std(list(d2.values()))\n",
    "    print(m2,sd2)\n",
    "    \n",
    "    for w in set(w for x in X for w in x):\n",
    "        d3[w] = 0.4*(d1[w] - m1) / sd1 + 0.6*(d2[w] - m2) / sd2\n",
    "\n",
    "    return d3\n",
    "\n",
    "wc_tr = get_counts_adv(X_raw_tr)\n",
    "# wc_val = get_counts_adv(X_raw_val)\n",
    "# wc_te = get_counts_adv(X_raw_te)\n",
    "\n",
    "# for w in words:\n",
    "#     if wc_tr[w]*wc_val[w]*wc_te[w]==0:\n",
    "#         del wc_tr[w]\n",
    "#         del wc_val[w]\n",
    "#         del wc_te[w]\n",
    "\n",
    "wordcounts = [len(x) for x in X_raw_tr]\n",
    "max_words = max(wordcounts)\n",
    "mean_words = int(np.mean(wordcounts))\n",
    "\n",
    "def get_most_frequent_keys(d,n):\n",
    "    a = np.array(list(d.values()),dtype='float')\n",
    "    N = min(n,len(a))\n",
    "    p = min(a[np.argpartition(a,-N)][-N:])\n",
    "    inds = a >= p\n",
    "    d = dict(np.array(list(d.items()))[inds])\n",
    "    for k in d:\n",
    "        d[k] = float(d[k])\n",
    "    return d\n",
    "\n",
    "vocab = get_most_frequent_keys(wc_tr, len(words)//10)\n",
    "print(len(vocab))\n",
    "\n",
    "def get_counts(i):\n",
    "    d = Counter()\n",
    "    for j in i:\n",
    "        d[j] += 1\n",
    "    return d\n",
    "\n",
    "runonsentence = [w if w in vocab else '<UNK>' for w in runonsentence]\n",
    "word_counts = get_counts(runonsentence)\n",
    "    \n",
    "#assert(min(list(word_counts.values())) > 1)\n",
    "\n",
    "vocabulary_size = len(word_counts)\n",
    "\n",
    "vocabulary_size, word_counts['<UNK>'], max_words, mean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.array([(0,len(x)) for x in X_raw_tr]+\n",
    "           [(1,len(x)) for x in X_raw_val]+\n",
    "           [(2,len(x)) for x in X_raw_te])\n",
    "T=np.vstack([T.T,np.arange(len(T))]).T\n",
    "tr=len(X_raw_tr)\n",
    "val=len(X_raw_val)\n",
    "T[:,2][tr:] -= tr\n",
    "T[:,2][tr+val:] -= val\n",
    "\n",
    "i = 0\n",
    "for t in T:\n",
    "    for j in range(t[1]):\n",
    "        if t[0] == 0:\n",
    "            X_raw_tr[t[2]][j] = runonsentence[i]\n",
    "        elif t[0] == 1:\n",
    "            X_raw_val[t[2]][j] = runonsentence[i]\n",
    "        else:\n",
    "            X_raw_te[t[2]][j] = runonsentence[i]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0, ..., 22, 14, 23],\n",
       "        [ 0,  0,  0, ..., 32, 33, 34],\n",
       "        [ 0,  0,  0, ..., 44, 45, 34],\n",
       "        [ 0,  0,  0, ..., 54, 55, 56],\n",
       "        [ 0,  0,  0, ..., 67, 73,  1]], dtype=int32),\n",
       " array([0, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "word2id = {word:i for i, word in enumerate(word_counts)}\n",
    "\n",
    "def convert_to_id(X_raw):\n",
    "    return [[word2id[word] for word in tweet] for tweet in X_raw]\n",
    "\n",
    "n_classes = len(set(y_raw_tr))\n",
    "assert(n_classes > 1)\n",
    "\n",
    "def convert_to_one_hot(y):\n",
    "#     return enc.transform(np.array(y_raw).reshape(-1,1)).toarray()\n",
    "    return y if n_classes == 2 else to_categorical(y, num_classes=n_classes)\n",
    "\n",
    "X_tr = sequence.pad_sequences(convert_to_id(X_raw_tr), maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(convert_to_id(X_raw_val), maxlen=max_words)\n",
    "X_te = sequence.pad_sequences(convert_to_id(X_raw_te), maxlen=max_words)\n",
    "\n",
    "y_tr = convert_to_one_hot(y_raw_tr)\n",
    "y_val = convert_to_one_hot(y_raw_val)\n",
    "y_te = convert_to_one_hot(y_raw_te)\n",
    "\n",
    "X_tr[:5], y_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.987841465340281, 1: 1.012461563359767}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=[0,1],\n",
    "                                                  y=y_tr)\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                   classes=np.arange(len(y_tr[0]))-1,\n",
    "#                                                   y=_y_tr)\n",
    "\n",
    "print(np.sum([y_tr==0])*class_weights[0]-np.sum(y_tr[y_tr==1])*class_weights[1])\n",
    "\n",
    "class_weights = {i:w for i,w in zip(range(len(y_tr)),class_weights)}\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 50, 51, 101, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "lstm_units = int(np.sqrt(vocabulary_size * embedding_size) / 4)\n",
    "\n",
    "num_samples = len(X_tr)\n",
    "\n",
    "dense_units = num_samples // 100\n",
    "\n",
    "units_a = lstm_units\n",
    "units_b_backward = lstm_units // 2\n",
    "units_b_forward = lstm_units - units_b_backward\n",
    "units_c = lstm_units\n",
    "units_d = dense_units\n",
    "\n",
    "dropout_dense = 0.2\n",
    "dropout_lstm = 1 - np.sqrt(1 - dropout_dense)\n",
    "dropout_bidir = 1 - np.sqrt(1 - dropout_lstm)\n",
    "\n",
    "num_classes = len(class_weights)\n",
    "\n",
    "units_a, units_b_backward, units_b_forward, units_c, units_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 32)          163584    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 101)         54136     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 101)         61612     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 101)               82012     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               51000     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 414,845\n",
      "Trainable params: 413,845\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import RandomUniform, GlorotUniform, Orthogonal\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Bidirectional, Dense,\n",
    "    Activation, BatchNormalization, Dropout\n",
    ")\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "L2 = l1_l2(l1=0, l2=5e-11)\n",
    "L1L2 = l1_l2(l1=1e-10, l2=5e-11)\n",
    "\n",
    "def lstm(units, return_sequences=True, dropout=dropout_lstm, go_backwards=False,\n",
    "         regularizer=L1L2, activation='tanh', recurrent_activation='sigmoid'):\n",
    "    return LSTM(units=units, return_sequences=return_sequences,\n",
    "                dropout=dropout, recurrent_dropout=dropout,\n",
    "                kernel_regularizer=regularizer, bias_regularizer=regularizer,\n",
    "                kernel_initializer=GlorotUniform(seed=getSeed()),\n",
    "                recurrent_initializer=Orthogonal(seed=getSeed()),\n",
    "                activation=activation, recurrent_activation=recurrent_activation,\n",
    "                go_backwards=go_backwards)\n",
    "\n",
    "inputs = Input(batch_input_shape=(None, None))\n",
    "\n",
    "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,\n",
    "              embeddings_initializer=RandomUniform(seed=getSeed()),\n",
    "              embeddings_regularizer=L2)(inputs)\n",
    "\n",
    "x = lstm(units_a)(x)\n",
    "\n",
    "forward_layer  = lstm(units_b_forward, dropout=dropout_bidir, regularizer=L2)\n",
    "backward_layer = lstm(units_b_backward, dropout=dropout_bidir, go_backwards=True)\n",
    "x = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(x)\n",
    "\n",
    "x = lstm(units_c, return_sequences=False)(x)\n",
    "\n",
    "x = Dense(units=units_d, kernel_regularizer=L1L2, bias_regularizer=L1L2,\n",
    "          kernel_initializer=GlorotUniform(seed=getSeed())\n",
    "         )(x)\n",
    "\n",
    "x = BatchNormalization(gamma_regularizer=L2, beta_regularizer=L2)(x)\n",
    "\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dropout(dropout_dense,seed=getSeed())(x)\n",
    "\n",
    "outputs = Dense(units=1 if num_classes == 2 else num_classes,\n",
    "                activation='sigmoid' if num_classes == 2 else 'softmax',\n",
    "                kernel_initializer=GlorotUniform(seed=getSeed()))(x)\n",
    "\n",
    "model=Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'\n",
    "model.compile(loss=loss, optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50048\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christophercoffee/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27/782 [>.............................] - ETA: 26:28 - loss: 0.6658 - accuracy: 0.5712"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = 'temp.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "num_epochs = 100\n",
    "print(num_samples)\n",
    "model.fit(X_tr,y_tr, validation_data=(X_val,y_val),\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          class_weight=class_weights,\n",
    "          callbacks=[checkpoint],\n",
    "          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_acc = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=1)\n",
    "\n",
    "yhat_val = np.squeeze(model.predict(X_val, batch_size=batch_size, verbose=1))\n",
    "\n",
    "y_true = y_val    # [[0, 1], [0, 0]]\n",
    "y_pred = yhat_val # [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "def get_batch_matrix(y):\n",
    "    y_matrix = []\n",
    "    for i in range(int(np.ceil(len(y) / batch_size))):\n",
    "        x = list(y[i*batch_size:(i+1)*batch_size])\n",
    "        if len(x) < batch_size:\n",
    "            x = x + ([np.nan] * (batch_size - len(x)))\n",
    "        y_matrix.append(tf.cast(x, tf.float32))\n",
    "    return tf.cast(y_matrix, tf.float32)\n",
    "y_true_matrix = get_batch_matrix(y_true)\n",
    "y_pred_matrix = get_batch_matrix(y_pred)\n",
    "\n",
    "model_loss, model_acc, yhat_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import Reduction\n",
    "\n",
    "calc_acc = 0\n",
    "for i in range(len(y_true)):\n",
    "    calc_acc += np.mean((np.array(y_pred[i]) > 0.5) * (np.array(y_true[i]) == 1) + \\\n",
    "                        (np.array(y_pred[i]) <= 0.5) * (np.array(y_true[i]) == 0))\n",
    "calc_acc = calc_acc / len(y_true)\n",
    "\n",
    "assert(calc_acc == K.mean(K.round(y_pred)==y_true))\n",
    "\n",
    "calc_acc - model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stable = K.clip(tf.cast(y_pred, tf.float32), K.epsilon(), 1-K.epsilon())\n",
    "logits = np.log(y_pred_stable / (1 - y_pred_stable))\n",
    "kbce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                          label_smoothing=K.epsilon(),\n",
    "                                          reduction=Reduction.SUM_OVER_BATCH_SIZE,\n",
    "                                          name='binary_crossentropy'\n",
    "                                         )(y_true, logits)\n",
    "kbce_logits = K.mean(kbce_logits)\n",
    "\n",
    "logits[y_true] *= class_weights[1]\n",
    "logits[~y_true] *= class_weights[0]\n",
    "kbce_logits2 = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                          label_smoothing=K.epsilon(),\n",
    "                                          reduction=Reduction.SUM_OVER_BATCH_SIZE,\n",
    "                                          name='binary_crossentropy'\n",
    "                                         )(y_true, logits)\n",
    "kbce_logits2 = K.mean(kbce_logits2)\n",
    "\n",
    "kbce = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=K.epsilon(),\n",
    "                                          reduction=Reduction.SUM_OVER_BATCH_SIZE,\n",
    "                                          name='binary_crossentropy'\n",
    "                                         )(y_true, y_pred)\n",
    "kbce = K.mean(kbce)\n",
    "kbce_logits,kbce_logits2,kbce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wbce(Y, P):\n",
    "    logloss = []\n",
    "    scewl_man = []\n",
    "    scewl_tf = []\n",
    "    for i in range(len(Y)):\n",
    "        Y_batch = tf.cast(Y[i], tf.float32)\n",
    "        P_batch = K.clip(tf.cast(P[i], tf.float32), K.epsilon(), 1-K.epsilon())\n",
    "        Z_batch = K.log(P_batch / (1 - P_batch))\n",
    "        logloss.append(-np.nanmean(Y_batch*K.log(P_batch)+(1-Y_batch)*K.log(1-P_batch)))\n",
    "        scewl_man.append(np.nanmean((1-Y_batch)*Z_batch+K.log(1+K.exp(-K.abs(Z_batch)))+K.maximum(-Z_batch,0)))\n",
    "        scewl_tf.append(np.nanmean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y_batch,\n",
    "                                                                       logits=Z_batch)))\n",
    "    kmean = lambda x: K.mean(tf.cast(x, tf.float32))\n",
    "    print((kmean(logloss) - kmean(scewl_man)))\n",
    "    print((kmean(logloss) - kmean(scewl_tf)))\n",
    "    return kmean(logloss)\n",
    "\n",
    "logloss = wbce(y_true_matrix, y_pred_matrix)\n",
    "logloss - kbce, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "confusion_matrix(y_true, np.round(y_pred)), \\\n",
    "precision_score(y_true, np.round(y_pred)), recall_score(y_true, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.round(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred, sample_weight=1):\n",
    "    if len(y_pred.shape)==1:\n",
    "        y_pred = np.atleast_2d(y_pred).T\n",
    "    y_pred = [max(min(pred[0], 1-K.epsilon()), K.epsilon()) for pred in y_pred]\n",
    "    y_true,y_pred,sample_weight = force_2d_shape([y_true,y_pred,sample_weight])\n",
    "\n",
    "    logits = np.log(y_pred) - np.log(1-y_pred) # sigmoid inverse\n",
    "    neg_abs_logits = -np.abs(logits)\n",
    "    relu_logits    = (logits > 0)*logits\n",
    "\n",
    "    loss_vec = relu_logits - logits*y_true + np.log(1 + np.exp(neg_abs_logits))\n",
    "    return np.mean(sample_weight*loss_vec)\n",
    "\n",
    "def force_2d_shape(arr_list):\n",
    "    for arr_idx, arr in enumerate(arr_list):\n",
    "        if len(np.array(arr).shape) != 2:\n",
    "            arr_list[arr_idx] = np.atleast_2d(arr).T\n",
    "    return arr_list\n",
    "\n",
    "def l1l2_weight_loss(model):\n",
    "    l1l2_loss = 0\n",
    "    for layer in model.layers:\n",
    "        if 'layer' in layer.__dict__ or 'cell' in layer.__dict__:\n",
    "            l1l2_loss += l1l2_rnn_loss(layer)\n",
    "            continue\n",
    "\n",
    "        if 'kernel_regularizer' in layer.__dict__ or \\\n",
    "           'bias_regularizer'   in layer.__dict__:\n",
    "            l1l2_lambda_k, l1l2_lambda_bias = [0,0], [0,0] # defaults\n",
    "            if layer.__dict__['kernel_regularizer'] is not None:\n",
    "                l1l2_lambda_k = list(layer.kernel_regularizer.__dict__.values())\n",
    "            if layer.__dict__['bias_regularizer']   is not None:\n",
    "                l1l2_lambda_bias = list(layer.bias_regularizer.__dict__.values())\n",
    "\n",
    "            l1l2_loss += compute_l1l2_losses(layer, l1l2_lambda_k + l1l2_lambda_bias)\n",
    "\n",
    "        if 'embeddings_regularizer' in layer.__dict__:\n",
    "            l1l2_lambda_e = [0,0]\n",
    "            if layer.__dict__['embeddings_regularizer'] is not None:\n",
    "                l1l2_lambda_e = list(layer.embeddings_regularizer.__dict__.values())\n",
    "                \n",
    "            l1l2_loss += compute_l1l2_losses(layer, l1l2_lambda_e)\n",
    "\n",
    "        if 'gamma_regularizer' in layer.__dict__ or \\\n",
    "           'beta_regularizer'  in layer.__dict__:\n",
    "            l1l2_lambda_g, l1l2_lambda_beta = [0,0], [0,0]\n",
    "            if layer.__dict__['gamma_regularizer'] is not None:\n",
    "                l1l2_lambda_g = list(layer.gamma_regularizer.__dict__.values())\n",
    "            if layer.__dict__['beta_regularizer']  is not None:\n",
    "                l1l2_lambda_beta = list(layer.beta_regularizer.__dict__.values())\n",
    "\n",
    "            l1l2_loss += compute_l1l2_losses(layer, l1l2_lambda_g + l1l2_lambda_beta)\n",
    "            \n",
    "    return l1l2_loss\n",
    "\n",
    "def l1l2_rnn_loss(layer):\n",
    "    l1l2_loss = 0\n",
    "    if 'backward_layer' in layer.__dict__:\n",
    "        forward_layer = layer.forward_layer\n",
    "        backward_layer = layer.backward_layer\n",
    "        bidirectional = True\n",
    "    else:\n",
    "        forward_layer = layer\n",
    "        bidirectional = False\n",
    "        \n",
    "    l1l2_loss += _l1l2_rnn_loss(forward_layer, bidirectional)\n",
    "    if bidirectional:\n",
    "        l1l2_loss += _l1l2_rnn_loss(backward_layer, bidirectional)\n",
    "\n",
    "    return l1l2_loss  \n",
    "\n",
    "def _l1l2_rnn_loss(layer, bidirectional):\n",
    "    ldict = layer.cell.__dict__\n",
    "\n",
    "    if 'kernel_regularizer'    in ldict or \\\n",
    "       'recurrent_regularizer' in ldict or \\\n",
    "       'bias_regularizer'      in ldict:\n",
    "        l1l2_lambda_k, l1l2_lambda_r, l1l2_lambda_bias = [0,0], [0,0], [0,0]\n",
    "        if ldict['kernel_regularizer']    is not None:\n",
    "            l1l2_lambda_k = list(layer.kernel_regularizer.__dict__.values())\n",
    "        if ldict['recurrent_regularizer'] is not None:\n",
    "            l1l2_lambda_r = list(layer.recurrent_regularizer.__dict__.values())\n",
    "        if ldict['bias_regularizer']      is not None:\n",
    "            l1l2_lambda_bias = list(layer.bias_regularizer.__dict__.values())\n",
    "\n",
    "        all_lambda = l1l2_lambda_k + l1l2_lambda_r + l1l2_lambda_bias\n",
    "        return compute_l1l2_losses(layer, all_lambda, bidirectional)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_l1l2_losses(layer, l1l2_lambda, bidirectional=False):\n",
    "    print(layer)\n",
    "    print(set([x if 'regularizer' in x else '' for x in layer.__dict__]))\n",
    "    \n",
    "    l1l2_loss = 0\n",
    "    if any([(_lambda != 0) for _lambda in l1l2_lambda]):\n",
    "        W = layer.get_weights()\n",
    "        if len(W) == 0: return 0\n",
    "        idx_incr = len(W)//2 # accounts for 'use_bias'\n",
    "\n",
    "        for idx,_lambda in enumerate(l1l2_lambda):\n",
    "            if _lambda != 0:\n",
    "                print(idx,_lambda)\n",
    "                _pow = 2**(idx % 2) # 1 if idx is even (l1), 2 if odd (l2)\n",
    "                l1l2_loss += _lambda*np.sum(np.abs(W[idx//2])**_pow)\n",
    "                print(l1l2_loss)\n",
    "                \n",
    "#                 if bidirectional:\n",
    "#                     l1l2_loss += _lambda*np.sum(\n",
    "#                                 np.abs(W[idx//2 + idx_incr])**_pow)\n",
    "#                     print(l1l2_loss)\n",
    "                    \n",
    "    return l1l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.array([class_weights[label] for label in y_val])\n",
    "custom_loss  = binary_crossentropy(y_true, y_pred,\n",
    "                                   sample_weight=sample_weights)\n",
    "print(custom_loss/kbce)\n",
    "custom_loss = kbce\n",
    "reg_losses = l1l2_weight_loss(model)\n",
    "print(reg_losses/custom_loss)\n",
    "custom_loss += reg_losses\n",
    "reg_losses/model_loss, custom_loss/model_loss-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
